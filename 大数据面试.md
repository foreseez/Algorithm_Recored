## ⼀.Hadoop

1）B树

B树中每个节点包含了键值和键值对于的数据对象存放地址指针，所以成功搜索一个对象可以不用到达树的叶节点。

成功搜索包括节点内搜索和沿某一路径的搜索，成功搜索时间取决于关键码所在的层次以及节点内关键码的数量。

在B树中查找给定关键字的方法是：首先把根结点取来，在根结点所包含的关键字K1,…,kj查找给定的关键字（可用顺序查找或二分查找法），若找到等于给定值的关键字，则查找成功；否则，一定可以确定要查的关键字在某个Ki或Ki+1之间，于是取Pi所指的下一层索引节点块继续查找，直到找到，或指针Pi为空时查找失败。

2）B+树

B+树非叶节点中存放的关键码并不指示数据对象的地址指针，非也节点只是索引部分。所有的叶节点在同一层上，包含了全部关键码和相应数据对象的存放地址指针，且叶节点按关键码从小到大顺序链接。如果实际数据对象按加入的顺序存储而不是按关键码次数存储的话，叶节点的索引必须是稠密索引，若实际数据存储按关键码次序存放的话，叶节点索引时稀疏索引。

B+树有2个头指针，一个是树的根节点，一个是最小关键码的叶节点。

所以 B+树有两种搜索方法：

一种是按叶节点自己拉起的链表顺序搜索。

一种是从根节点开始搜索，和B树类似，不过如果非叶节点的关键码等于给定值，搜索并不停止，而是继续沿右指针，一直查到叶节点上的关键码。所以无论搜索是否成功，都将走完树的所有层。

B+ 树中，数据对象的插入和删除仅在叶节点上进行。

这两种处理索引的数据结构的不同之处：
a，B树中同一键值不会出现多次，并且它有可能出现在叶结点，也有可能出现在非叶结点中。而B+树的键一定会出现在叶结点中，并且有可能在非叶结点中也有可能重复出现，以维持B+树的平衡。
b，因为B树键位置不定，且在整个树结构中只出现一次，虽然可以节省存储空间，但使得在插入、删除操作复杂度明显增加。B+树相比来说是一种较好的折中。
c，B树的查询效率与键在树中的位置有关，最大时间复杂度与B+树相同(在叶结点的时候)，最小时间复杂度为1(在根结点的时候)。而B+树的时候复杂度对某建成的树是固定的。

### 1.hdfs写流程

N. 客户端跟namenode通信请求上传⽂件，namenode检查⽬标⽂件是否已存在，⽗⽬

录是否存在

O. namenode返回是否可以上传

P. client请求第⼀个 block该传输到哪些datanode服务器上

Q. namenode返回3个datanode服务器ABC

S. client请求3台dn中的⼀台A上传数据（本质上是⼀个RPC调⽤，建⽴pipeline），A

收到请求会继续调⽤B，然后B调⽤C，将真个pipeline建⽴完成，逐级返回客户端

T. client开始往A上传第⼀个block（先从磁盘读取数据放到⼀个本地内存缓存），以

packet为单位，A收到⼀个packet就会传给B，B传给C；A每传⼀个packet会放⼊⼀

个应答队列等待应答

V. 当⼀个block传输完成之后，client再次请求namenode上传第⼆个block的服务器。

### 2.hdfs读流程

N. client跟namenode通信查询元数据，找到⽂件块所在的datanode服务器

O. 挑选⼀台datanode（就近原则，然后随机）服务器，请求建⽴socket流

P. datanode开始发送数据（从磁盘⾥⾯读取数据放⼊流，以packet为单位来做校验）

Q. 客户端以packet为单位接收，现在本地缓存，然后写⼊⽬标⽂件

### 3.hdfs的体系结构

hdfs有namenode、secondraynamenode、datanode组成。为n+1模式

N. NameNode负责管理和记录整个⽂件系统的元数据

O. DataNode 负责管理⽤户的⽂件数据块，⽂件会按照固定的⼤⼩（blocksize）切成

若⼲块后分布式存储在若⼲台datanode上，每⼀个⽂件块可以有多个副本，并存放

在不同的datanode上，Datanode会定期向Namenode汇报⾃身所保存的⽂件block

信息，⽽namenode则会负责保持⽂件的副本数量

P. HDFS的内部⼯作机制对客户端保持透明，客户端请求访问HDFS都是通过向

namenode申请来进⾏

Q. secondraynamenode负责合并⽇志

### 4.⼀个datanode 宕机,怎么⼀个流程恢复

Datanode宕机了后，如果是短暂的宕机，可以实现写好脚本监控，将它启动起来。如果

是⻓时间宕机了，那么datanode上的数据应该已经被备份到其他机器了，那这台

datanode就是⼀台新的datanode了，删除他的所有数据⽂件和状态⽂件，重新启动。

### 5.hadoop 的 namenode 宕机,怎么解决

先分析宕机后的损失，宕机后直接导致client⽆法访问，内存中的元数据丢失，但是硬盘

中的元数据应该还存在，如果只是节点挂了，重启即可，如果是机器挂了，重启机器后看

节点是否能重启，不能重启就要找到原因修复了。但是最终的解决⽅案应该是在设计集群

的初期就考虑到这个问题，做namenode的HA。

### 6.namenode对元数据的管理

namenode对数据的管理采⽤了三种存储形式：

内存元数据(NameSystem)

磁盘元数据镜像⽂件(fsimage镜像)

数据操作⽇志⽂件（可通过⽇志运算出元数据）(edit⽇志⽂件)

### 7.元数据的checkpoint

每隔⼀段时间，会由secondary namenode将namenode上积累的所有edits和⼀个最新的

fsimage下载到本地，并加载到内存进⾏merge（这个过程称为checkpoint）

namenode和secondary namenode的⼯作⽬录存储结构完全相同，所以，当namenode

故障退出需要重新恢复时，可以从secondary namenode的⼯作⽬录中将fsimage拷⻉到

namenode的⼯作⽬录，以恢复namenode的元数据

### 8.yarn资源调度流程

N. ⽤户向YARN 中提交应⽤程序， 其中包括ApplicationMaster 程序、启动

ApplicationMaster 的命令、⽤户程序等。

O. ResourceManager 为该应⽤程序分配第⼀个Container， 并与对应的NodeManager

通信，要求它在这个Container 中启动应⽤程序的ApplicationMaster。

P. ApplicationMaster ⾸先向ResourceManager 注册， 这样⽤户可以直接通过

ResourceManage 查看应⽤程序的运⾏状态，然后它将为各个任务申请资源，并监

控它的运⾏状态，直到运⾏结束，即重复步骤4~7。

Q. ApplicationMaster 采⽤轮询的⽅式通过RPC 协议向ResourceManager 申请和领取

资源。

S. ⼀旦ApplicationMaster 申请到资源后，便与对应的NodeManager 通信，要求它启

动任务。

T. NodeManager 为任务设置好运⾏环境（包括环境变量、JAR 包、⼆进制程序等）

后，将任务启动命令写到⼀个脚本中，并通过运⾏该脚本启动任务。

V. 各个任务通过某个RPC 协议向ApplicationMaster 汇报⾃⼰的状态和进度，以让

ApplicationMaster 随时掌握各个任务的运⾏状态，从⽽可以在任务失败时重新启动

任务。在应⽤程序运⾏过程中，⽤户可随时通过RPC 向ApplicationMaster 查询应⽤

程序的当前运⾏状态。

W. 应⽤程序运⾏完成后，ApplicationMaster 向ResourceManager 注销并关闭⾃⼰。

### 9.hadoop中combiner和partition的作⽤

combiner是发⽣在map的最后⼀个阶段，⽗类就是Reducer，意义就是对每⼀个

maptask的输出进⾏局部汇总，以减⼩⽹络传输量，缓解⽹络传输瓶颈，提⾼

reducer的执⾏效率。

partition的主要作⽤将map阶段产⽣的所有kv对分配给不同的reducer task处理，可

以将reduce阶段的处理负载进⾏分摊

### 10.⽤mapreduce怎么处理数据倾斜问题？

数据倾斜：map /reduce程序执⾏时，reduce节点⼤部分执⾏完毕，但是有⼀个或者⼏个

reduce节点运⾏很慢，导致整个程序的处理时间很⻓，这是因为某⼀个key的条数⽐其他

key多很多（有时是百倍或者千倍之多），这条key所在的reduce节点所处理的数据量⽐

其他节点就⼤很多，从⽽导致某⼏个节点迟迟运⾏不完，此称之为数据倾斜。

（1）局部聚合加全局聚合。

第⼀次在 map 阶段对那些导致了数据倾斜的 key 加上 1 到 n 的随机前缀，这样本来相

同的 key 也会被分到多个 Reducer 中进⾏局部聚合，数量就会⼤⼤降低。

第⼆次 mapreduce，去掉 key 的随机前缀，进⾏全局聚合。

思想：⼆次 mr，第⼀次将 key 随机散列到不同 reducer 进⾏处理达到负载均衡⽬的。第

⼆次再根据去掉 key 的随机前缀，按原 key 进⾏ reduce 处理。

这个⽅法进⾏两次 mapreduce，性能稍差。

（2）增加 Reducer，提升并⾏度

JobConf.setNumReduceTasks(int)

（3）实现⾃定义分区

根据数据分布情况，⾃定义散列函数，将 key 均匀分配到不同 Reducer

### 11.shuffle 阶段,你怎么理解的

shuffle: 洗牌、发牌——（核⼼机制：缓存，数据分区，排序，Merge进⾏局部value的合

并）；

具体来说：就是将maptask输出的处理结果数据，分发给reducetask，并在分发的过程

中，对数据按key进⾏了分区和排序；

1）Map ⽅法之后 Reduce ⽅法之前这段处理过程叫 Shuffle

2）Map ⽅法之后，数据⾸先进⼊到分区⽅法，把数据标记好分区，然后把数据发送到 环

形缓冲区；环形缓冲区默认⼤⼩ 100m，环形缓冲区达到 80%时，进⾏溢写；溢写前对数

据进⾏排序，排序按照对 key 的索引进⾏字典顺序排序，排序的⼿段快排；溢写产⽣⼤量

溢 写⽂件，需要对溢写⽂件进⾏归并排序；对溢写的⽂件也可以进⾏ Combiner 操作，

前提是汇总操作，求平均值不⾏。最后将⽂件按照分区存储到磁盘，等待 Reduce 端拉

取。

3）每个 Reduce 拉取 Map 端对应分区的数据。拉取数据后先存储到内存中，内存不够

了，再存储到磁盘。拉取完所有数据后，采⽤归并排序将内存和磁盘中的数据都进⾏排

序。

在进⼊ Reduce ⽅法前，可以对数据进⾏分组操作。

### 12.Mapreduce 的 map 数量 和 reduce 数量是由什么决定的 ,怎么配置

map的数量由输⼊切⽚的数量决定，128M切分⼀个切⽚，只要是⽂件也分为⼀个切⽚，

有多少个切⽚就有多少个map Task。

reduce数量⾃⼰配置。

### 13.MapReduce优化经验

N. 设置合理的map和reduce的个数。合理设置blocksize

O. 避免出现数据倾斜

P. combine函数

Q. 对数据进⾏压缩

S. ⼩⽂件处理优化：事先合并成⼤⽂件，combineTextInputformat，在hdfs上⽤

mapreduce将⼩⽂件合并成SequenceFile⼤⽂件（key:⽂件名，value：⽂件内容）

T. 参数优化

### 14.分别举例什么情况要使⽤ combiner，什么情况不使⽤？

求平均数的时候就不需要⽤combiner，因为不会减少reduce执⾏数量。在其他的时候，

可以依据情况，使⽤combiner，来减少map的输出数量，减少拷⻉到reduce的⽂件，从

⽽减轻reduce的压⼒，节省⽹络开销，提升执⾏效率

### 15.MR运⾏流程解析

N. ⼀个mr程序启动的时候，最先启动的是MRAppMaster，MRAppMaster启动后根据

本次job的描述信息，计算出需要的maptask实例数量，然后向集群申请机器启动相

应数量的maptask进程

O. maptask进程启动之后，根据给定的数据切⽚范围进⾏数据处理，主体流程为：

a. 利⽤客户指定的inputformat来获取RecordReader读取数据，形成输⼊KV对

b. 将输⼊KV对传递给客户定义的map()⽅法，做逻辑运算，并将map()⽅法输出的

KV对收集到缓存

c. 将缓存中的KV对按照K分区排序后不断溢写到磁盘⽂件

P. MRAppMaster监控到所有maptask进程任务完成之后，会根据客户指定的参数启动

相应数量的reducetask进程，并告知reducetask进程要处理的数据范围（数据分

区）

Q. Reducetask进程启动之后，根据MRAppMaster告知的待处理数据所在位置，从若⼲

台maptask运⾏所在机器上获取到若⼲个maptask输出结果⽂件，并在本地进⾏重新

归并排序，然后按照相同key的KV为⼀个组，调⽤客户定义的reduce()⽅法进⾏逻辑

运算，并收集运算输出的结果KV，然后调⽤客户指定的outputformat将结果数据输

出到外部存储

### 16.简单描述⼀下HDFS的系统架构，怎么保证数据安全?

2020/7/15 2020⼤数据⾯试题真题总结(附答案)

https://mp.weixin.qq.com/s/Lo3zH5fbLs60ZhzDtTtHFw 14/59

HDFS数据安全性如何保证？

N. 存储在HDFS系统上的⽂件，会分割成128M⼤⼩的block存储在不同的节点上，

block的副本数默认3份，也可配置成更多份；

O. 第⼀个副本⼀般放置在与client（客户端）所在的同⼀节点上（若客户端⽆

datanode，则随机放），第⼆个副本放置到与第⼀个副本同⼀机架的不同节点，第

三个副本放到不同机架的datanode节点，当取⽤时遵循就近原则；

P. datanode已block为单位，每3s报告⼼跳状态，做10min内不报告⼼跳状态则

namenode认为block已死掉，namonode会把其上⾯的数据备份到其他⼀个

datanode节点上，保证数据的副本数量；

Q. datanode会默认每⼩时把⾃⼰节点上的所有块状态信息报告给namenode；

S. 采⽤safemode模式：datanode会周期性的报告block信息。Namenode会计算block

的损坏率，当阀值<0.999f时系统会进⼊安全模式，HDFS只读不写。HDFS元数据采

### 18.Hadoop优化有哪些⽅⾯

0）HDFS ⼩⽂件影响

（1）影响 NameNode 的寿命，因为⽂件元数据存储在 NameNode 的内存中

（2）影响计算引擎的任务数量，⽐如每个⼩的⽂件都会⽣成⼀个 Map 任务

1）数据输⼊⼩⽂件处理：

（1）合并⼩⽂件：对⼩⽂件进⾏归档（Har）、⾃定义 Inputformat 将⼩⽂件存储成

SequenceFile ⽂件。

（2）采⽤ ConbinFileInputFormat 来作为输⼊，解决输⼊端⼤量⼩⽂件场景。

（3）对于⼤量⼩⽂件 Job，可以开启 JVM 重⽤。

2）Map 阶段

（1）增⼤环形缓冲区⼤⼩。由 100m 扩⼤到 200m

（2）增⼤环形缓冲区溢写的⽐例。由 80%扩⼤到 90%

（3）减少对溢写⽂件的 merge 次数。（10 个⽂件，⼀次 20 个 merge）

（4）不影响实际业务的前提下，采⽤ Combiner 提前合并，减少 I/O。

3）Reduce 阶段

（1）合理设置 Map 和 Reduce 数：两个都不能设置太少，也不能设置太多。太少，会导

致 Task 等待，延⻓处理时间；太多，会导致 Map、Reduce 任务间竞争资源，造成处理

超时等错误。

（2）设置 Map、Reduce 共存：调整 slowstart.completedmaps 参数，使 Map 运⾏到

⼀定程度后，Reduce 也开始运⾏，减少 Reduce 的等待时间。

（3）规避使⽤ Reduce，因为 Reduce 在⽤于连接数据集的时候将会产⽣⼤量的⽹络消

耗。

（4）增加每个 Reduce 去 Map 中拿数据的并⾏数

（5）集群性能可以的前提下，增⼤ Reduce 端存储数据内存的⼤⼩。

4）IO 传输

（1）采⽤数据压缩的⽅式，减少⽹络 IO 的的时间。安装 Snappy 和 LZOP 压缩编码器。

（2）使⽤ SequenceFile ⼆进制⽂件

5）整体

（1）MapTask 默认内存⼤⼩为 1G，可以增加 MapTask 内存⼤⼩为 4-5g

（2）ReduceTask 默认内存⼤⼩为 1G，可以增加 ReduceTask 内存⼤⼩为 4-5g

（3）可以增加 MapTask 的 cpu 核数，增加 ReduceTask 的 CPU 核数

（4）增加每个 Container 的 CPU 核数和内存⼤⼩

（5）调整每个 Map Task 和 Reduce Task 最⼤重试次数

19.⼤量数据求topN(写出mapreduce的实现思路）

### 20.列出正常⼯作的hadoop集群中hadoop都分别启动哪些进程以及他们的作⽤

1.NameNode它是hadoop中的主服务器，管理⽂件系统名称空间和对集群中存储的⽂件

的访问，保存有metadate。

2.SecondaryNameNode它不是namenode的冗余守护进程，⽽是提供周期检查点和清理

任务。帮助NN合并editslog，减少NN启动时间。

3.DataNode它负责管理连接到节点的存储（⼀个集群中可以有多个节点）。每个存储数

据的节点运⾏⼀个datanode守护进程。

4.ResourceManager（JobTracker）JobTracker负责调度DataNode上的⼯作。每个

DataNode有⼀个TaskTracker，它们执⾏实际⼯作。

5.NodeManager（TaskTracker）执⾏任务

2020/7/15 2020⼤数据⾯试题真题总结(附答案)

https://mp.weixin.qq.com/s/Lo3zH5fbLs60ZhzDtTtHFw 16/59

6.DFSZKFailoverController⾼可⽤时它负责监控NN的状态，并及时的把状态信息写⼊

ZK。它通过⼀个独⽴线程周期性的调⽤NN上的⼀个特定接⼝来获取NN的健康状态。FC

也有选择谁作为Active NN的权利，因为最多只有两个节点，⽬前选择策略还⽐较简单

（先到先得，轮换）。

7.JournalNode ⾼可⽤情况下存放namenode的editlog⽂件.

### 21.Hadoop总job和Tasks之间的区别是什么？

Job是我们对⼀个完整的mapreduce程序的抽象封装

Task是job运⾏时，每⼀个处理阶段的具体实例，如map task，reduce task，maptask和

reduce task都会有多个并发运⾏的实例

### 22.Hadoop⾼可⽤HA模式

HDFS⾼可⽤原理：

Hadoop HA（High Available）通过同时配置两个处于Active/Passive模式的Namenode

来解决上述问题，状态分别是Active和Standby. Standby Namenode作为热备份，从⽽

允许在机器发⽣故障时能够快速进⾏故障转移，同时在⽇常维护的时候使⽤优雅的⽅式进

⾏Namenode切换。Namenode只能配置⼀主⼀备，不能多于两个Namenode。

主Namenode处理所有的操作请求（读写），⽽Standby只是作为slave，维护尽可能同步

的状态，使得故障时能够快速切换到Standby。为了使Standby Namenode与Active

Namenode数据保持同步，两个Namenode都与⼀组Journal Node进⾏通信。当主

Namenode进⾏任务的namespace操作时，都会确保持久会修改⽇志到Journal Node节

点中。Standby Namenode持续监控这些edit，当监测到变化时，将这些修改同步到⾃⼰

的namespace。

当进⾏故障转移时，Standby在成为Active Namenode之前，会确保⾃⼰已经读取了

Journal Node中的所有edit⽇志，从⽽保持数据状态与故障发⽣前⼀致。

为了确保故障转移能够快速完成，Standby Namenode需要维护最新的Block位置信息，

即每个Block副本存放在集群中的哪些节点上。为了达到这⼀点，Datanode同时配置主

备两个Namenode，并同时发送Block报告和⼼跳到两台Namenode。

确保任何时刻只有⼀个Namenode处于Active状态⾮常重要，否则可能出现数据丢失或者

数据损坏。当两台Namenode都认为⾃⼰的Active Namenode时，会同时尝试写⼊数据

（不会再去检测和同步数据）。为了防⽌这种脑裂现象，Journal Nodes只允许⼀个

Namenode写⼊数据，内部通过维护epoch数来控制，从⽽安全地进⾏故障转移。

### 23.简要描述安装配置⼀个hadoop集群的步骤

N. 使⽤root账户登录。

O. 修改IP。

P. 修改Host主机名。

Q. 配置SSH免密码登录。

S. 关闭防⽕墙。

T. 安装JDK。

V. 上传解压Hadoop安装包。

W. 配置Hadoop的核⼼配置⽂件hadoop-evn.sh，core-site.xml，mapred-site.xml，

hdfs-site.xml，yarn-site.xml

X. 配置hadoop环境变量

NL. 格式化hdfs # bin/hadoop namenode -format

NN. 启动节点start-all.sh

### 24.fsimage和edit的区别

fsimage：filesystem image 的简写，⽂件镜像。

客户端修改⽂件时候，先更新内存中的metadata信息,只有当对⽂件操作成功的时候，才

会写到editlog。

fsimage是⽂件meta信息的持久化的检查点。secondary namenode会定期的将fsimage

和editlog合并dump成新的fsimage

25.yarn的三⼤调度策略

FIFO Scheduler把应⽤按提交的顺序排成⼀个队列，这是⼀个先进先出队列，在进⾏资源

分配的时候，先给队列中最头上的应⽤进⾏分配资源，待最头上的应⽤需求满⾜后再给下

⼀个分配，以此类推。

Capacity（容量）调度器，有⼀个专⻔的队列⽤来运⾏⼩任务，但是为⼩任务专⻔设置⼀

个队列会预先占⽤⼀定的集群资源，这就导致⼤任务的执⾏时间会落后于使⽤FIFO调度器

时的时间。

在Fair（公平）调度器中，我们不需要预先占⽤⼀定的系统资源，Fair调度器会为所有运

⾏的job动态的调整系统资源。当第⼀个⼤job提交时，只有这⼀个job在运⾏，此时它获

得了所有集群资源；当第⼆个⼩任务提交后，Fair调度器会分配⼀半资源给这个⼩任务，

让这两个任务公平的共享集群资源。

需要注意的是，在下图Fair调度器中，从第⼆个任务提交到获得资源会有⼀定的延迟，

因为它需要等待第⼀个任务释放占⽤的Container。⼩任务执⾏完成之后也会释放⾃⼰占

⽤的资源，⼤任务⼜获得了全部的系统资源。最终的效果就是Fair调度器即得到了⾼的资

源利⽤率⼜能保证⼩任务及时完成。

26.hadoop的shell命令⽤的多吗?,说出⼀些常⽤的

## ⼆.Hive

### 1.⼤表join⼩表产⽣的问题，怎么解决？

mapjoin⽅案

join因为空值导致⻓尾(key为空值是⽤随机值代替)

join因为热点值导致⻓尾，也可以将热点数据和⾮热点数据分开处理，最后合并

### 2.udf udaf udtf区别

UDF操作作⽤于单个数据⾏，并且产⽣⼀个数据⾏作为输出。⼤多数函数都属于这⼀类

（⽐如数学函数和字符串函数）。

UDAF 接受多个输⼊数据⾏，并产⽣⼀个输出数据⾏。像COUNT和MAX这样的函数就是

聚集函数。

UDTF 操作作⽤于单个数据⾏，并且产⽣多个数据⾏-------⼀个表作为输出。lateral

view explore()

简单来说：

UDF:返回对应值，⼀对⼀

UDAF：返回聚类值，多对⼀

UDTF：返回拆分值，⼀对多

### 3.hive有哪些保存元数据的⽅式，个有什么特点。

内存数据库derby，安装⼩，但是数据存在内存，不稳定

mysql数据库，数据存储模式可以⾃⼰设置，持久化好，查看⽅便。

### 4.hive内部表和外部表的区别

内部表：加载数据到hive所在的hdfs⽬录，删除时，元数据和数据⽂件都删除

外部表：不加载数据到hive所在的hdfs⽬录，删除时，只删除表结构。

这样外部表相对来说更加安全些，数据组织也更加灵活，⽅便共享源数据。

### 5.⽣产环境中为什么建议使⽤外部表？

N. 因为外部表不会加载数据到hive，减少数据传输、数据还能共享。

O. hive不会修改数据，所以⽆需担⼼数据的损坏

P. 删除表时，只删除表结构、不删除数据。

### 6.insert into 和 override write区别？

insert into：将数据写到表中

override write：覆盖之前的内容。

### 7.hive的判断函数有哪些

hive 的条件判断（if、coalesce、case）

### 8.简单描述⼀下HIVE的功能？⽤hive创建表有⼏种⽅式？hive表有⼏种？

hive主要是做离线分析的

hive建表有三种⽅式

直接建表法

查询建表法(通过AS 查询语句完成建表：将⼦查询的结果存在新表⾥，有数据，⼀般

⽤于中间表)

like建表法(会创建结构完全相同的表，但是没有数据)

hive表有2种：内部表和外部表

### 9.线上业务每天产⽣的业务⽇志（压缩后>=3G），每天需要加载到hive的log表中，将每天产

⽣的业务⽇志在压缩之后load到hive的log表时，最好使⽤的压缩算法是哪个,并说明其原因

10.若在hive中建⽴分区仍不能优化查询效率，建表时如何优化

### 11.union all和union的区别

union 去重

union oll 不去重

### 12.如何解决hive数据倾斜的问题

1）group by

注：group by 优于 distinct group

情形：group by 维度过⼩，某值的数量过多

后果：处理某值的 reduce ⾮常耗时

解决⽅式：采⽤ sum() group by 的⽅式来替换 count(distinct)完成计算。

2）count(distinct)

count(distinct xx)

情形：某特殊值过多

后果：处理此特殊值的 reduce 耗时；只有⼀个 reduce 任务

解决⽅式：count distinct 时，将值为空的情况单独处理，⽐如可以直接过滤空值的⾏，

在最后结果中加 1。如果还有其他计算，需要进⾏ group by，可以先将值为空的记录单独

处

理，再和其他计算结果进⾏ union。

3）mapjoin

4）不同数据类型关联产⽣数据倾斜

情形：⽐如⽤户表中 user_id 字段为 int，log 表中 user_id 字段既有 string 类型也有 int

类型。当按照 user_id 进⾏两个表的 Join 操作时。

后果：处理此特殊值的 reduce 耗时；只有⼀个 reduce 任务

默认的 Hash 操作会按 int 型的 id 来进⾏分配，这样会导致所有 string 类型 id 的记录都

分配

到⼀个 Reducer 中。

解决⽅式：把数字类型转换成字符串类型

select * from users a

left outer join logs b

on a.usr_id = cast(b.user_id as string)

5）开启数据倾斜时负载均衡

set hive.groupby.skewindata=true;

思想：就是先随机分发并处理，再按照 key group by 来分发处理。

操作：当选项设定为 true，⽣成的查询计划会有两个 MRJob。

第⼀个 MRJob 中，Map 的输出结果集合会随机分布到 Reduce 中，每个 Reduce 做部分

聚合操作，并输出结果，这样处理的结果是相同的 GroupBy Key 有可能被分发到不同的

Reduce 中，从⽽达到负载均衡的⽬的；

第⼆个 MRJob 再根据预处理的数据结果按照 GroupBy Key 分布到 Reduce 中（这个过

程可以保证相同的原始 GroupBy Key 被分布到同⼀个 Reduce 中），最后完成最终的聚

合操

作。

点评：它使计算变成了两个 mapreduce，先在第⼀个中在 shuffle 过程 partition 时随机

给 key 打标记，使每个 key 随机均匀分布到各个 reduce 上计算，但是这样只能完成部分

计算，因为相同 key 没有分配到相同 reduce 上。

所以需要第⼆次的 mapreduce,这次就回归正常 shuffle,但是数据分布不均匀的问题在第

⼀次 mapreduce 已经有了很⼤的改善，因此基本解决数据倾斜。因为⼤量计算已经在第

⼀次

mr 中随机分布到各个节点完成。

6）控制空值分布

将为空的 key 转变为字符串加随机数或纯随机数，将因空值⽽造成倾斜的数据分不到多

个 Reducer。

注：对于异常值如果不需要的话，最好是提前在 where 条件⾥过滤掉，这样可以使计算

量⼤⼤减少

### 13.hive性能优化常⽤的⽅法

1）MapJoin

如果不指定 MapJoin 或者不符合 MapJoin 的条件，那么 Hive 解析器会将 Join 操作转换

成 Common Join，即：在 Reduce 阶段完成 join。容易发⽣数据倾斜。可以⽤ MapJoin

把⼩

表全部加载到内存在 map 端进⾏ join，避免 reducer 处理。

2）⾏列过滤

列处理：在 SELECT 中，只拿需要的列，如果有，尽量使⽤分区过滤，少⽤ SELECT *。

⾏处理：在分区剪裁中，当使⽤外关联时，如果将副表的过滤条件写在 Where 后⾯，那

么就会先全表关联，之后再过滤。

3）列式存储

4）采⽤分区技术

5）合理设置 Map 数

（1）通常情况下，作业会通过 input 的⽬录产⽣⼀个或者多个 map 任务。

主要的决定因素有：input 的⽂件总个数，input 的⽂件⼤⼩，集群设置的⽂件块⼤⼩。

（2）是不是 map 数越多越好？

答案是否定的。如果⼀个任务有很多⼩⽂件（远远⼩于块⼤⼩ 128m），则每个⼩⽂件

也会被当做⼀个块，⽤⼀个 map 任务来完成，⽽⼀个 map 任务启动和初始化的时间远远

⼤

于逻辑处理的时间，就会造成很⼤的资源浪费。⽽且，同时可执⾏的 map 数是受限的。

（3）是不是保证每个 map 处理接近 128m 的⽂件块，就⾼枕⽆忧了？

答案也是不⼀定。⽐如有⼀个 127m 的⽂件，正常会⽤⼀个 map 去完成，但这个⽂件只

有⼀个或者两个⼩字段，却有⼏千万的记录，如果 map 处理的逻辑⽐较复杂，⽤⼀个

map

任务去做，肯定也⽐较耗时。

针对上⾯的问题 2 和 3，我们需要采取两种⽅式来解决：即减少 map 数和增加 map 数；

6）⼩⽂件进⾏合并

在 Map 执⾏前合并⼩⽂件，减少 Map 数：CombineHiveInputFormat 具有对⼩⽂件进

⾏

合并的功能（系统默认的格式）。HiveInputFormat 没有对⼩⽂件合并功能。

7）合理设置 Reduce 数

Reduce 个数并不是越多越好

（1）过多的启动和初始化 Reduce 也会消耗时间和资源；

（2）另外，有多少个 Reduce，就会有多少个输出⽂件，如果⽣成了很多个⼩⽂件，那

么如果这些⼩⽂件作为下⼀个任务的输⼊，则也会出现⼩⽂件过多的问题；

在设置 Reduce 个数的时候也需要考虑这两个原则：处理⼤数据量利⽤合适的 Reduce

数；使单个 Reduce 任务处理数据量⼤⼩要合适；

8）常⽤参数

// 输出合并⼩⽂件

SET hive.merge.mapfiles = true; -- 默认 true，在 map-only 任务结束时合并

⼩⽂件

SET hive.merge.mapredfiles = true; -- 默认 false，在 map-reduce 任务结

束时合并⼩⽂件

SET hive.merge.size.per.task = 268435456; -- 默认 256M

SET hive.merge.smallfiles.avgsize = 16777216; -- 当输出⽂件的平均⼤⼩

⼩于 16m 该值时，启动⼀个独⽴的 map-reduce 任务进⾏⽂件 merge

9）开启 map 端 combiner（不影响最终业务逻辑）

set hive.map.aggr=true；

10）压缩（选择快的）

设置 map 端输出、中间结果压缩。（不完全是解决数据倾斜的问题，但是减少了 IO 读

写和⽹络传输，能提⾼很多效率）

11）开启 JVM 重⽤

### 14.简述delete，drop，truncate的区别

delet 删除数据

drop 删除表

truncate 摧毁表结构并重建

### 15.四个by的区别

N. Sort By：分区内有序；

O. Order By：全局排序，只有⼀个 Reducer；

P. Distrbute By：类似 MR 中 Partition，进⾏分区，结合 sort by 使⽤。

Q. Cluster By：当 Distribute by 和 Sorts by 字段相同时，可以使⽤ Cluster by ⽅式。

Cluster by 除了具有 Distribute by 的功能外还兼具 Sort by 的功能。但是排序只能

是升序排序，不能 指定排序规则为 ASC 或者 DESC。

### 16.Hive ⾥边字段的分隔符⽤的什么？为什么⽤\t？有遇到过字段⾥ 边有\t 的情况吗，怎么处

理的？为什么不⽤ Hive 默认的分隔符，默认的分隔符是什么？

hive 默认的字段分隔符为 ascii 码的控制符Ä001（^A）,建表的时候⽤ fields terminated

by 'Ä001'

遇到过字段⾥边有Ät 的情况，⾃定义 InputFormat，替换为其他分隔符再做后续处理

### 17.分区分桶的区别，为什么要分区

分区表：原来的⼀个⼤表存储的时候分成不同的数据⽬录进⾏存储。如果说是单分区表，

那么在表的⽬录下就只有⼀级⼦⽬录，如果说是多分区表，那么在表的⽬录下有多少分区

2020/7/15 2020⼤数据⾯试题真题总结(附答案)

https://mp.weixin.qq.com/s/Lo3zH5fbLs60ZhzDtTtHFw 22/59

就有多少级⼦⽬录。不管是单分区表，还是多分区表，在表的⽬录下，和⾮最终分区⽬录

下是不能直接存储数据⽂件的

分桶表：原理和hashpartitioner ⼀样，将hive中的⼀张表的数据进⾏归纳分类的时候，

归纳分类规则就是hashpartitioner。（需要指定分桶字段，指定分成多少桶）

分区表和分桶的区别除了存储的格式不同外，最主要的是作⽤：

分区表：细化数据管理，缩⼩mapreduce程序 需要扫描的数据量。

分桶表：提⾼join查询的效率，在⼀份数据会被经常⽤来做连接查询的时候建⽴分

桶，分桶字段就是连接字段；提⾼采样的效率。

分区表和分桶的区别除了存储的格式不同外，最主要的是作⽤：

分区表：细化数据管理，缩⼩mapreduce程序 需要扫描的数据量。

分桶表：提⾼join查询的效率，在⼀份数据会被经常⽤来做连接查询的时候建⽴分

桶，分桶字段就是连接字段；提⾼采样的效率。

### 18.mapjoin的原理

MapJoin通常⽤于⼀个很⼩的表和⼀个⼤表进⾏join的场景，具体⼩表有多⼩，由

参数hive.mapjoin.smalltable.filesize来决定，该参数表示⼩表的总⼤⼩，默认值为

25000000字节，即25M。

Hive0.7之前，需要使⽤hint提示 /*+ mapjoin(table) */才会执⾏MapJoin,否则执⾏

Common Join ， 但在0.7 版本之后， 默认⾃动会转换Map Join ， 由参数

hive.auto.convert.join来控制，默认为true.

2020/7/15 2020⼤数据⾯试题真题总结(附答案)

https://mp.weixin.qq.com/s/Lo3zH5fbLs60ZhzDtTtHFw 23/59

假设a表为⼀张⼤表，b为⼩表，并且hive.auto.convert.join=true,那么Hive在执⾏时候会

⾃动转化为MapJoin。

MapJoin简单说就是在Map阶段将⼩表读⼊内存，顺序扫描⼤表完成Join。减少昂贵的shuffle操作

及reduce操作

MapJoin分为两个阶段：

通过MapReduce Local Task，将⼩表读⼊内存，⽣成HashTableFiles上传⾄Distributed

Cache中，这⾥会HashTableFiles进⾏压缩。

MapReduce Job在Map阶段，每个Mapper从Distributed Cache读取HashTableFiles到内存

中，顺序扫描⼤表，在Map阶段直接进⾏Join，将数据传递给下⼀个MapReduce任务。

### 19.在hive的row_number中distribute by 和 partition by的区别



### 22.hive都有哪些函数，你平常⼯作中⽤到哪些

数学函数

round(DOUBLE a)

floor(DOUBLE a)

ceil(DOUBLE a)

rand()

集合函数

size(Map<K.V>)

map_keys(Map<K.V>)

map_values(Map<K.V>)

array_contains(Array<T>, value)

sort_array(Array<T>)

类型转换函数

cast(expr as <type>)

⽇期函数

date_format函数（根据格式整理⽇期）

date_add、date_sub函数（加减⽇期）

next_day函数

last_day函数（求当⽉最后⼀天⽇期）

collect_set函数

get_json_object解析json函数

2020/7/15 2020⼤数据⾯试题真题总结(附答案)

https://mp.weixin.qq.com/s/Lo3zH5fbLs60ZhzDtTtHFw 24/59

from_unixtime(bigint unixtime, string format)

to_date(string timestamp)

year(string date)

month(string date)

hour(string date)

weekofyear(string date)

datediff(string enddate, string startdate)

add_months(string start_date, int num_months)

date_format(date/timestamp/string ts, string fmt)

条件函数

if(boolean testCondition, T valueTrue, T valueFalseOrNull)

nvl(T value, T default_value)

COALESCE(T v1, T v2, ...)

CASE a WHEN b THEN c [WHEN d THEN e]* [ELSE f] END

isnull( a )

isnotnull ( a )

字符函数

concat(string|binary A, string|binary B...)

concat_ws(string SEP, string A, string B...)

get_json_object(string json_string, string path)

length(string A)

lower(string A) lcase(string A)

parse_url(string urlString, string partToExtract [, string keyToExtract])

regexp_replace(string INITIAL_STRING, string PATTERN, string REPLACEMENT)

reverse(string A)

split(string str, string pat)

substr(string|binary A, int start) substring(string|binary A, int start)

聚合函数

count sum min max avg

表⽣成函数

explode(array< ů Ćc> a)

explode(ARRAY)

json_tuple(jsonStr, k1, k2, ...)

parse_url_tuple(url, p1, p2, ...)

## 三.Spark

- Spark提供了内存计算，可将中间结果放到内存中，对于迭代运算效率更高
- Spark基于DAG的任务调度执行机制，要优于Hadoop MapReduce的迭代执行机制
- 使用多线程池模型来减少task启动开稍，shuffle过程中避免 不必要的sort操作以及减少磁盘IO操作，而MapReduce使用多进程模型
- Spark提供了多种语言支持，特别是scala，方便易用

## Spark重点概念

- **Application**: Appliction都是指用户编写的Spark应用程序，其中包括一个`Driver功能`的代码和分布在集群中多个节点上运行的`Executor代码`

- **Driver**:  Spark中的Driver即运行上述Application的`main函数`并创建`SparkContext`，创建SparkContext的目的是为了准备Spark应用程序的运行环境，在Spark中有SparkContext负责与ClusterManager通信，进行资源申请、任务的分配和监控等，当Executor部分运行完毕后，Driver同时负责将SparkContext关闭，通常用**SparkContext代表Driver**

- **Executor**:  某个Application运行在`worker节点`上的一个`进程`，该进程负责运行某些Task，并且负责将数据存到内存或磁盘上，每个Application都有`各自独立的一批Executor`， 在Spark on Yarn模式下，其进程名称为CoarseGrainedExecutor Backend。一个CoarseGrainedExecutor Backend有且仅有一个Executor对象， 负责将Task包装成taskRunner,并从线程池中抽取一个空闲线程运行Task， 这个每一个CoarseGrainedExecutor Backend能并行运行Task的数量取决与分配给它的cpu个数

- Cluter Manager

  ：指的是在集群上获取资源的外部服务。目前有三种类型

  - **Standalon** : spark原生的资源管理，由Master负责资源的分配
  - **Apache Mesos**:与hadoop MR兼容性良好的一种资源调度框架
  - **Hadoop Yarn**: 主要是指Yarn中的ResourceManager

- **Worker**: 集群中任何可以运行Application代码的节点，在Standalone模式中指的是通过slave文件配置的Worker节点，在Spark on Yarn模式下就是NodeManager节点

- **Task**: 被送到某个Executor上的工作单元，和hadoopMR中的MapTask和ReduceTask概念一样，是运行Application的基本单位，多个Task组成一个Stage，而Task的调度和管理等是由TaskScheduler负责

- **Job**: 包含多个Task组成的并行计算，往往由Spark Action触发生成， **一个Application中往往会产生多个Job**

- **Stage**: 每个Job会被拆分成多组Task， 作为一个TaskSet， 其名称为Stage，Stage的划分和调度是有DAGScheduler来负责的，Stage有非最终的Stage（Shuffle Map Stage）和最终的Stage（Result Stage）两种，Stage的边界就是发生shuffle的地方

- **DAGScheduler**: 根据Job构建基于Stage的DAG（Directed Acyclic Graph有向无环图)，并提交Stage给TASkScheduler。 其划分Stage的依据是RDD之间的依赖的关系找出开销最小的调度方法，如下图



![img](https://user-gold-cdn.xitu.io/2019/3/29/169c8c72f92100ce?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)



- **TASKSedulter**: 将TaskSET提交给worker运行，每个Executor运行什么Task就是在此处分配的. TaskScheduler维护所有TaskSet，当Executor向Driver发生心跳时，TaskScheduler会根据资源剩余情况分配相应的Task。另外TaskScheduler还维护着所有Task的运行标签，重试失败的Task。下图展示了TaskScheduler的作用



![img](https://user-gold-cdn.xitu.io/2019/3/29/169c8c74d7477302?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)



- 在不同运行模式中任务调度器具体为：
  - Spark on Standalone模式为TaskScheduler
  - YARN-Client模式为YarnClientClusterScheduler
  - YARN-Cluster模式为YarnClusterScheduler 将这些术语串起来的运行层次图如下：



![img](https://user-gold-cdn.xitu.io/2019/3/29/169c8c809d183551?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)



简单总结：**Job=多个stage，Stage=多个同种task, Task分为ShuffleMapTask和ResultTask，Dependency分为ShuffleDependency(宽依赖)和NarrowDependency(窄依赖)**

> - ResultTask：对于 DAG 图中最后一个 Stage（也就是 ResultStage），会生成与该 DAG 图中哦最后一个 RDD （DAG 图中最后边）partition 个数相同的 ResultTask
> - ShuffleMapTask：对于非最后的 Stage（也就是 ShuffleMapStage），会生成与该 Stage 最后的 RDD partition 个数相同的 ShuffleMapTask

## Spark运行模式

Spark的运行模式多种多样，灵活多变，部署在单机上时，既可以用本地模式运行，也可以用伪分布模式运行，而当以分布式集群的方式部署时，也有众多的运行模式可供选择，这取决于集群的实际情况，底层的资源调度即可以依赖外部资源调度框架，主要有以下3种：

- local本地模式
- Spark内建的Standalone模式。
- Spark on Yarn模式
- Spark on mesos模式

### local模式

此种模式下，我们只要将Spark包解压即可使用，运行时Spark目录下的bin目录执行bin/spark-shell即可，适合自己练习以及测试的情况。

> [Spark运行模式（一）－－－－－Spark独立模式](https://blog.csdn.net/happyanger6/article/details/47070223)

### Standalone模式

> [Spark(一): 基本架构及原理](http://www.cnblogs.com/tgzhu/p/5818374.html)

- Standalone模式使用Spark自带的资源调度框架
- 采用Master/Slaves的典型架构，选用ZooKeeper来实现Master的HA
- 框架结构图如下:



![img](https://user-gold-cdn.xitu.io/2019/3/31/169d1d5dba6078bb?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)



- 该模式主要的节点有Client节点、Master节点和Worker节点。**其中Driver既可以运行在Master节点上中，也可以运行在本地Client端**。当用spark-shell交互式工具提交Spark的Job时，Driver在Master节点上运行；当使用spark-submit工具提交Job或者在Eclips、IDEA等开发平台上使用”new SparkConf.setManager(“spark://master:7077”)”方式运行Spark任务时，Driver是运行在本地Client端上的 运行过程如下图：



![img](https://user-gold-cdn.xitu.io/2019/3/31/169d1d5feee06818?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)



1. SparkContext连接到Master，**向Master注册并申请资源**（CPU Core 和Memory）
2. Master根据SparkContext的资源申请要求和Worker心跳周期内报告的信息决定在哪个Worker上分配资源，然后在该Worker上**获取资源**，然后启动StandaloneExecutorBackend；
3. StandaloneExecutorBackend**向SparkContext注册**；
4. SparkContext将Applicaiton代码**发送给StandaloneExecutorBackend**；并且SparkContext解析Applicaiton代码，**构建DAG图**，并提交给DAG Scheduler**分解成Stage**（当碰到Action操作时，就会催生Job；每个Job中含有1个或多个Stage，Stage一般在获取外部数据和shuffle之前产生），然后以Stage（或者称为TaskSet）**提交给Task Scheduler**，Task Scheduler负责将Task**分配到相应的Worker**，最后**提交给StandaloneExecutorBackend执行**；
5. StandaloneExecutorBackend会**建立Executor线程池，开始执行Task**，并向SparkContext报告，直至Task完成
6. 所有Task完成后，SparkContext向Master**注销**，释放资源

### Spark on Yarn模式

Yarn的基本框架可以阅读前面的文章[Hadoop学习（二）——MapReduce\Yarn架构](https://juejin.im/post/6844903779972677639)。
 Spark on YARN模式根据Driver在集群中的位置分为两种模式：一种是YARN-Client模式，另一种是YARN-Cluster（或称为YARN-Standalone模式）。

#### Yarn Client模式

- Yarn-Client模式中，**Driver在客户端本地运行**，这种模式可以使得Spark Application和客户端进行交互，因为Driver在客户端，所以可以通过webUI访问Driver的状态，默认是http://hadoop1:4040访问，而YARN通过http:// hadoop1:8088访问
- YARN-client的工作流程步骤为：



![img](https://user-gold-cdn.xitu.io/2019/3/31/169d219ad2221338?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)



1. Spark Yarn Client向YARN的ResourceManager**申请启动Application Master**。同时在SparkContent初始化中将创建DAGScheduler和TASKScheduler等，由于我们选择的是Yarn-Client模式，程序会选择YarnClientClusterScheduler和YarnClientSchedulerBackend
2. ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container中**启动应用程序的ApplicationMaster**，**与YARN-Cluster区别的是在该ApplicationMaster不运行SparkContext，只与SparkContext进行联系进行资源的分派**
3. Client中的SparkContext初始化完毕后，**与ApplicationMaster建立通讯，向ResourceManager注册**，根据任务信息向ResourceManager**申请资源（Container）**
4. 一旦ApplicationMaster申请到资源（也就是Container）后，便**与对应的NodeManager通信**，要求它在获得的Container中**启动CoarseGrainedExecutorBackend**，CoarseGrainedExecutorBackend启动后会向Client中的SparkContext**注册并申请Task**
5. client中的SparkContext**分配Task给CoarseGrainedExecutorBackend执行**，CoarseGrainedExecutorBackend运行Task并**向Driver 汇报运行的状态和进度**，以让Client随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务
6. 应用程序运行完成后，Client的SparkContext向ResourceManager申请**注销**并关闭自己

#### YARN-Cluster模式

- 在YARN-Cluster模式中，当用户向YARN中提交一个应用程序后，YARN将分两个阶段运行该应用程序：
  1. 第一个阶段是把Spark的Driver作为一个ApplicationMaster在YARN集群中先启动；
  2. 第二个阶段是由ApplicationMaster创建应用程序，然后为它向ResourceManager申请资源，并启动Executor来运行Task，同时监控它的整个运行过程，直到运行完成
- YARN-cluster的工作流程分为以下几个步骤



![img](https://user-gold-cdn.xitu.io/2019/3/31/169d21e988f76e03?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)



1. Spark Yarn Client向YARN中提交应用程序，包括ApplicationMaster程序、启动ApplicationMaster的命令、需要在Executor中运行的程序等
2. ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster，其中**ApplicationMaster进行SparkContext等(Driver)的初始化**
3. ApplicationMaster向ResourceManager**注册**，这样用户可以直接通过ResourceManage查看应用程序的运行状态，然后它将采用轮询的方式通过RPC协议为各个任务**申请资源**，并监控它们的运行状态直到运行结束
4. 一旦ApplicationMaster申请到资源（也就是Container）后，便与对应的NodeManager**通信**，要求它在获得的Container中**启动CoarseGrainedExecutorBackend**，CoarseGrainedExecutorBackend启动后会向ApplicationMaster中的SparkContext**注册并申请Task**。这一点和Standalone模式一样，只不过SparkContext在Spark Application中初始化时，使用CoarseGrainedSchedulerBackend配合YarnClusterScheduler进行任务的调度，其中YarnClusterScheduler只是对TaskSchedulerImpl的一个简单包装，增加了对Executor的等待逻辑等
5. ApplicationMaster中的SparkContext**分配Task给CoarseGrainedExecutorBackend执行**，CoarseGrainedExecutorBackend运行Task并向ApplicationMaster**汇报运行的状态和进度**，以让ApplicationMaster随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务
6. 应用程序运行完成后，ApplicationMaster向ResourceManager申请**注销**并关闭自己

#### Spark Client 和 Spark Cluster的区别:

- 理解YARN-Client和YARN-Cluster深层次的区别之前先清楚一个概念：Application Master。在YARN中，每个Application实例都有一个ApplicationMaster进程，它是**Application启动的第一个容器**。它负责和ResourceManager打交道并请求资源，获取资源之后告诉NodeManager为其启动Container。从深层次的含义讲YARN-Cluster和YARN-Client模式的区别其实就是ApplicationMaster进程的区别
- YARN-Cluster模式下，**Driver运行在AM(Application Master)中**，它负责向YARN申请资源，并监督作业的运行状况。当用户提交了作业之后，就可以关掉Client，作业会继续在YARN上运行，因而YARN-Cluster模式不适合运行交互类型的作业
- YARN-Client模式下，**Driver运行在Client客户端中**，Application Master仅仅向YARN请求Executor，Client会和请求的Container通信来调度他们工作，也就是说Client不能离开

## Spark的运行流程

### Spark的基本运行流程

spark运行流程图如下：



![img](https://user-gold-cdn.xitu.io/2019/3/31/169d2295d8a590f7?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)



1. 构建Spark Application的运行环境，启动SparkContext
2. SparkContext向资源管理器（可以是Standalone，Mesos，Yarn）申请运行Executor资源，并启动StandaloneExecutorbackend
3. Executor向SparkContext申请Task
4. SparkContext将应用程序分发给Executor
5. SparkContext构建成DAG图，将DAG图分解成Stage、将Taskset发送给Task Scheduler，最后由Task Scheduler将Task发送给Executor运行
6. Task在Executor上运行，运行完释放所有资源

### RDD的基本运行流程

- RDD在Spark中运行大概分为以下三步：
  1. 创建RDD对象
  2. DAGScheduler模块介入运算，计算RDD之间的依赖关系，RDD之间的依赖关系就形成了DAG
  3. 每一个Job被分为多个Stage。划分Stage的一个主要依据是当前计算因子的输入是否是确定的（宽依赖窄依赖），如果是则将其分在同一个Stage，避免多个Stage之间的消息传递开销
- 示例图如下：



![img](https://user-gold-cdn.xitu.io/2019/3/31/169d236b7a610eee?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)

以下面一个按 A-Z 首字母分类，查找相同首字母下不同姓名总个数的例子来看一下 RDD 是如何运行起来的





![img](https://user-gold-cdn.xitu.io/2019/3/31/169d236f0e746aeb?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)

创建 RDD  上面的例子除去最后一个 collect 是个动作，不会创建 RDD 之外，前面四个转换都会创建出新的 RDD 。因此第一步就是创建好所有 RDD( 内部的五项信息 )？ 创建执行计划 Spark 会尽可能地管道化，并基于是否要重新组织数据来划分 阶段 (stage) ，例如本例中的 groupBy() 转换就会将整个执行计划划分成两阶段执行。最终会产生一个 DAG(directed acyclic graph ，有向无环图 ) 作为逻辑执行计划





![img](https://user-gold-cdn.xitu.io/2019/3/31/169d23723750b65e?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)

调度任务  将各阶段划分成不同的 任务 (task) ，每个任务都是数据和计算的合体。在进行下一阶段前，当前阶段的所有任务都要执行完成。因为下一阶段的第一个转换一定是重新组织数据的，所以必须等当前阶段所有结果数据都计算出来了才能继续

### 1.rdd的属性

⼀组分⽚（Partition），即数据集的基本组成单位。对于RDD来说，每个分⽚都会被

⼀个计算任务处理，并决定并⾏计算的粒度。⽤户可以在创建RDD时指定RDD的分

⽚个数，如果没有指定，那么就会采⽤默认值。默认值就是程序所分配到的CPU

Core的数⽬。

⼀个计算每个分区的函数。Spark中RDD的计算是以分⽚为单位的，每个RDD都会实

现compute函数以达到这个⽬的。compute函数会对迭代器进⾏复合，不需要保存

每次计算的结果。

RDD之间的依赖关系。RDD的每次转换都会⽣成⼀个新的RDD，所以RDD之间就会

形成类似于流⽔线⼀样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这

个依赖关系重新计算丢失的分区数据，⽽不是对RDD的所有分区进⾏重新计算。

⼀个Partitioner，即RDD的分⽚函数。当前Spark中实现了两种类型的分⽚函数，⼀

个是基于哈希的HashPartitioner，另外⼀个是基于范围的RangePartitioner。只有对

于于key-value的RDD，才会有Partitioner，⾮key-value的RDD的Parititioner的值

是None。Partitioner函数不但决定了RDD本身的分⽚数量，也决定了parent RDD

Shuffle输出时的分⽚数量。

⼀个列表，存储存取每个Partition的优先位置（preferred location）。对于⼀个

HDFS⽂件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据

不如移动计算”的理念，Spark在进⾏任务调度的时候，会尽可能地将计算任务分配

到其所要处理数据块的存储位置。

### 2.算⼦分为哪⼏类(RDD⽀持哪⼏种类型的操作)

转换（Transformation） 现有的RDD通过转换⽣成⼀个新的RDD。lazy模式，延迟执

⾏。

转换函数包括： map ， filter ， flatMap ， groupByKey ， reduceByKey ，

aggregateByKey，union,join, coalesce 等等。

动作（Action） 在RDD上运⾏计算，并返回结果给驱动程序(Driver)或写⼊⽂件系统。

动作操作包括：reduce，collect，count，first，take，countByKey以及foreach等等。

collect 该⽅法把数据收集到driver端 Array数组类型

所有的transformation只有遇到action才能被执⾏。

当触发执⾏action之后，数据类型不再是rdd了，数据就会存储到指定⽂件系统中，或者

直接打印结 果或者收集起来。

### 3.创建rdd的⼏种⽅式

#### 1.集合并⾏化创建(有数据)

val arr = Array(1,2,3,4,5)

val rdd = sc.parallelize(arr)

val rdd =sc.makeRDD(arr)

#### 2.读取外部⽂件系统，如hdfs，或者读取本地⽂件(最常⽤的⽅式)(没数据)

val rdd2 = sc.textFile("hdfs://hdp-01:9000/words.txt")

// 读取本地⽂件

val rdd2 = sc.textFile(“file:///root/words.txt”)

#### 3.从⽗RDD转换成新的⼦RDD

调⽤Transformation类的⽅法，⽣成新的RDD

#### 4.spark运⾏流程

Worker的功能：定时和master通信；调度并管理⾃身的executor

executor：由Worker启动的，程序最终在executor中运⾏，（程序运⾏的⼀个容器）

spark-submit命令执⾏时，会根据master地址去向 Master发送请求，

Master接收到Dirver端的任务请求之后，根据任务的请求资源进⾏调度，（打散的策

略），尽可能的 把任务资源平均分配，然后向WOrker发送指令

Worker收到Master的指令之后，就根据相应的资源，启动executor（cores,memory）

executor会向dirver端建⽴请求，通知driver，任务已经可以运⾏了

driver运⾏任务的时候，会把任务发送到executor中去运⾏。

#### 5.Spark中coalesce与repartition的区别

1）关系：

两者都是⽤来改变 RDD 的 partition 数量的，repartition 底层调⽤的就是 coalesce ⽅

法：coalesce(numPartitions, shuffle = true)

2）区别：

repartition ⼀定会发⽣ shuffle，coalesce 根据传⼊的参数来判断是否发⽣ shuffle

⼀般情况下增⼤ rdd 的 partition 数量使⽤ repartition，减少 partition 数量时使⽤

coalesce

#### 6.sortBy 和 sortByKey的区别

sortBy既可以作⽤于RDD[K] ，还可以作⽤于RDD[(k,v)]

sortByKey 只能作⽤于 RDD[K,V] 类型上。

7.map和mapPartitions的区别

#### 8.数据存⼊Redis 优先使⽤map mapPartitions foreach foreachPartions哪个

使⽤ foreachPartition

\* 1,map mapPartition 是转换类的算⼦， 有返回值

\* 2, 写mysql,redis 的连接

foreach * 100万 100万次的连接

foreachPartions * 200 个分区 200次连接 ⼀个分区中的数据，共⽤⼀个连接

foreachParititon 每次迭代⼀个分区，foreach每次迭代⼀个元素。

该⽅法没有返回值，或者Unit

主要作⽤于，没有返回值类型的操作（打印结果，写⼊到mysql数据库中）

在写⼊到redis,mysql的时候，优先使⽤foreachPartititon

#### 9.reduceByKey和groupBykey的区别

reduceByKey会传⼀个聚合函数， 相当于 groupByKey + mapValues

reduceByKey 会有⼀个分区内聚合，⽽groupByKey没有 最核⼼的区别

结论：reduceByKey有分区内聚合，更⾼效，优先选择使⽤reduceByKey。

#### 10.cache和checkPoint的⽐较

都是做 RDD 持久化的

1.缓存，是在触发action之后，把数据写⼊到内存或者磁盘中。不会截断⾎缘关系

（设置缓存级别为memory_only：内存不⾜，只会部分缓存或者没有缓存，缓存会丢

失,memory_and_disk :内存不⾜，会使⽤磁盘）

2.checkpoint 也是在触发action之后，执⾏任务。单独再启动⼀个job，负责写⼊数据到

hdfs中。（把rdd中的数据，以⼆进制⽂本的⽅式写⼊到hdfs中，有⼏个分区，就有⼏个

⼆进制⽂件）

3.某⼀个RDD被checkpoint之后，他的⽗依赖关系会被删除，⾎缘关系被截断，该RDD转

换成了CheckPointRDD，以后再对该rdd的所有操作，都是从hdfs中的checkpoint的具体

⽬录来读取数据。缓存之后，rdd的依赖关系还是存在的。

#### 11.spark streaming流式统计单词数量代码

object WordCountAll {

// newValues当前批次的出现的单词次数， runningCount表示之前运⾏的单词出现的结果

/* def updateFunction(newValues: Seq[Int], runningCount: Option[Int]):

val newCount = newValues.sum + runningCount.getOrElse(0)// 将历史前⼏

Some(newCount)

}*/

/**

\* String : 单词 hello

\* Seq[Int] ：单词在当前批次出现的次数

\* Option[Int] ： 历史结果

*/

val updateFunc = (iter: Iterator[(String, Seq[Int], Option[Int])]) =>

//iter.flatMap(it=>Some(it._2.sum + it._3.getOrElse(0)).map(x=>(it._

iter.flatMap{case(x,y,z)=>Some(y.sum + z.getOrElse(0)).map(m=>(x, m

}

// 屏蔽⽇志

Logger.getLogger("org.apache").setLevel(Level.ERROR)

def main(args: Array[String]) {

// 必须要开启2个以上的线程，⼀个线程⽤来接收数据，另外⼀个线程⽤来计算

val conf = new SparkConf().setMaster("local[2]").setAppName("Network

// 设置sparkjob计算时所采⽤的序列化⽅式

.set("spark.serializer", "org.apache.spark.serializer.KryoSerializ

.set("spark.rdd.compress", "true") // 节约⼤量的内存内容

#### 12.简述map和flatMap的区别和应⽤场景

map是对每⼀个元素进⾏操作，flatmap是对每⼀个元素操作后并压平

#### 14.分别列出⼏个常⽤的transformation和action算⼦

转换算⼦：map,map,filter,reduceByKey,groupByKey,groupBy

⾏动算⼦： foreach ，

foreachpartition,collect,collectAsMap,take,top,first,count,countByKey

#### 15.按照需求使⽤spark编写以下程序，要求使⽤scala语⾔

当前⽂件a.txt的格式，请统计每个单词出现的次数

// 同时也会创建sparkContext对象

// 批次时间 >= 批次处理的总时间 (批次数据量，集群的计算节点数量和配置)

val ssc = new StreamingContext(conf, Seconds(5))

//做checkpoint 写⼊共享存储中

ssc.checkpoint("c://aaa")

// 创建⼀个将要连接到 hostname:port 的 DStream，如 localhost:9999

val lines: ReceiverInputDStream[String] = ssc.socketTextStream("192

//updateStateByKey结果可以累加但是需要传⼊⼀个⾃定义的累加函数：updateFunc

val results = lines.flatMap(_.split(" ")).map((_,1)).updateStateByKe

//打印结果到控制台

results.print()

//开始计算

ssc.start()

//等待停⽌

ssc.awaitTermination()

}

}

#### 16.spark应⽤程序的执⾏命令是什么？

/usr/local/spark-current2.3/bin/spark-submit Ä

--class com.wedoctor.Application Ä

--master yarn Ä

--deploy-mode client Ä

--driver-memory 1g Ä

--executor-memory 2g Ä

--queue root.wedw Ä

--num-executors 200 Ä

--jars /home/pgxl/liuzc/config-1.3.0.jar,/home/pgxl/liuzc/hadoop-lzo-

0.4.20.jar,/home/pgxl/liuzc/elasticsearch-hadoop-hive-2.3.4.jar Ä

/home/pgxl/liuzc/sen.jar

#### 17.Spark应⽤执⾏有哪些模式，其中哪⼏种是集群模式

本地local模式

standalone模式

spark on yarn模式

spark on mesos模式

其中，standalone模式，spark on yarn模式，spark on mesos模式是集群模式

#### 18.请说明spark中⼴播变量的⽤途

使⽤⼴播变量，每个 Executor 的内存中，只驻留⼀份变量副本，⽽不是对 每个 task 都

传输⼀次⼤变量，省了很多的⽹络传输， 对性能提升具有很⼤帮助， ⽽且会通过⾼效的

object WordCount {

def main(args: Array[String]): Unit = {

val conf = new SparkConf()

.setAppName(this.getClass.getSimpleName)

.setMaster("local[*]")

val sc = new SparkContext(conf)

var sData: RDD[String] = sc.textFile("a.txt")

val sortData: RDD[(String, Int)] = sData.flatMap(_.split(",")).map((

sortData.foreach(print)

}

}

#### 20.写出你⽤过的spark中的算⼦，其中哪些会产⽣shuffle过程

reduceBykey：

groupByKey：

…ByKey:

#### 23.描述⼀下RDD，DataFrame，DataSet的区别？

1）RDD

优点:

编译时类型安全

编译时就能检查出类型错误

⾯向对象的编程⻛格

直接通过类名点的⽅式来操作数据

缺点:

序列化和反序列化的性能开销

⽆论是集群间的通信, 还是 IO 操作ˇˇˇˇˇˇˇˇ都需要对对象的结构和数据进⾏序列化和反序列化。

GC 的性能开销，频繁的创建和销毁对象, 势必会增加 GC

2）DataFrame

DataFrame 引⼊了 schema 和 off-heap

schema : RDD 每⼀⾏的数据, 结构都是⼀样的，这个结构就存储在 schema 中。Spark

通过 schema 就能够读懂数据, 因此在通信和 IO 时就只需要序列化和反序列化数据, ⽽结

构的部分就可以省略了。

3）DataSet

DataSet 结合了 RDD 和 DataFrame 的优点，并带来的⼀个新的概念 Encoder。

当序列化数据时，Encoder 产⽣字节码与 off-heap 进⾏交互，能够达到按需访问数据的

效果，⽽不⽤反序列化整个对象。Spark 还没有提供⾃定义 Encoder 的 API，但是未来会

加⼊。

### 27.RDD中的数据在哪？

RDD中的数据在数据源，RDD只是⼀个抽象的数据集，我们通过对RDD的操作就相当于对

数据进⾏操作。

### 28.如果对RDD进⾏cache操作后，数据在哪⾥？

数据在第⼀执⾏cache算⼦时会被加载到各个Executor进程的内存中，第⼆次就会直接从

内存中读取⽽不会区磁盘。

### 29.Spark中Partition的数量由什么决定

和Mr⼀样，但是Spark默认最少有两个分区。

### 30.Scala⾥⾯的函数和⽅法有什么区别

31.SparkStreaming怎么进⾏监控?

32.Spark判断Shuffle的依据?

⽗RDD的⼀个分区中的数据有可能被分配到⼦RDD的多个分区中

33.Scala有没有多继承？可以实现多继承么？

34.Sparkstreaming和flink做实时处理的区别

35.Sparkcontext的作⽤

36.Sparkstreaming读取kafka数据为什么选择直连⽅式

37.离线分析什么时候⽤sparkcore和sparksql

38.Sparkstreaming实时的数据不丢失的问题

39.简述宽依赖和窄依赖概念，groupByKey,reduceByKey,map,filter,union五种操作哪些会导

43.共享变量和累加器

累加器（accumulator）是 Spark 中提供的⼀种分布式的变量机制，其原理类似于

mapreduce，即分布式的改变，然后聚合这些改变。累加器的⼀个常⻅⽤途是在调试时对

作业执⾏过程中的事件进⾏计数。⽽⼴播变量⽤来⾼效分发较⼤的对象。

共享变量出现的原因：

通常在向 Spark 传递函数时，⽐如使⽤ map() 函数或者⽤ filter() 传条件时，可以使⽤驱

动器程序中定义的变量，但是集群中运⾏的每个任务都会得到这些变量的⼀份新的副本，

更新这些副本的值也不会影响驱动器中的对应变量。

Spark 的两个共享变量，累加器与⼴播变量，分别为结果聚合与⼴播这两种常⻅的通信模

式突破了这⼀限制。

44.当 Spark 涉及到数据库的操作时，如何减少 Spark 运⾏中的数据库连接数？

使⽤ foreachPartition 代替 foreach，在 foreachPartition 内获取数据库的连接。

45.特别⼤的数据，怎么发送到excutor中？

46.spark调优都做过哪些⽅⾯？

47.spark任务为什么会被yarn kill掉？

48.Spark on Yarn作业执⾏流程？yarn-client和yarn-cluster有什么区别？

Spark on Yarn作业执⾏流程？

1.Spark Yarn Client 向 Yarn 中提交应⽤程序。

2.ResourceManager 收到请求后，在集群中选择⼀个 NodeManager，并为该应⽤程序

分配⼀个 Container，在这个 Container 中启动应⽤程序的 ApplicationMaster，

ApplicationMaster 进⾏ SparkContext 等的初始化。

3.ApplicationMaster 向 ResourceManager 注册， 这样⽤户可以直接通过

ResourceManager 查看应⽤程序的运⾏状态，然后它将采⽤轮询的⽅式通过RPC协议为

各个任务申请资源，并监控它们的运⾏状态直到运⾏结束。

4.ApplicationMaster 申请到资源（也就是Container）后，便与对应的 NodeManager

通信，并在获得的 Container 中启动 CoarseGrainedExecutorBackend，启动后会向

ApplicationMaster 中的 SparkContext 注册并申请 Task。

5.ApplicationMaster 中的 SparkContext 分配 Task 给

CoarseGrainedExecutorBackend 执⾏，CoarseGrainedExecutorBackend 运⾏ Task

并向ApplicationMaster 汇报运⾏的状态和进度，以让 ApplicationMaster 随时掌握各个

任务的运⾏状态，从⽽可以在任务失败时重新启动任务。

6.应⽤程序运⾏完成后，ApplicationMaster 向 ResourceManager申请注销并关闭⾃

⼰。

yarn-client和yarn-cluster有什么区别？

1.理解YARN-Client和YARN-Cluster深层次的区别之前先清楚⼀个概念：Application

Master。在YARN中，每个Application实例都有⼀个ApplicationMaster进程，它是

Application启动的第⼀个容器。它负责和ResourceManager打交道并请求资源，获取资

源之后告诉NodeManager为其启动Container。从深层次的含义讲YARN-Cluster和

YARN-Client模式的区别其实就是ApplicationMaster进程的区别 2. YARN-Cluster模式

下，Driver运⾏在AM(Application Master)中，它负责向YARN申请资源，并监督作业的

运⾏状况。当⽤户提交了作业之后，就可以关掉Client，作业会继续在YARN上运⾏，因

⽽YARN-Cluster模式不适合运⾏交互类型的作业 3. YARN-Client模式下，Application

Master仅仅向YARN请求Executor，Client会和请求的Container通信来调度他们⼯作，也

就是说Client不能离开



四.Kafka

1.Kafka名词解释和⼯作⽅式

N. Producer ：消息⽣产者，就是向kafka broker发消息的客户端。

O. Consumer ：消息消费者，向kafka broker取消息的客户端

P. Topic ：咋们可以理解为⼀个队列。

Q. Consumer Group （CG）：这是kafka⽤来实现⼀个topic消息的⼴播（发给所有的

consumer）和单播（发给任意⼀个consumer）的⼿段。⼀个topic可以有多个CG。

topic的消息会复制（不是真的复制，是概念上的）到所有的CG，但每个partion只会

把消息发给该CG中的⼀个consumer。如果需要实现⼴播，只要每个consumer有⼀

个独⽴的CG就可以了。要实现单播只要所有的consumer在同⼀个CG。⽤CG还可以

将consumer进⾏⾃由的分组⽽不需要多次发送消息到不同的topic。

S. Broker ：⼀台kafka服务器就是⼀个broker。⼀个集群由多个broker组成。⼀个

broker可以容纳多个topic。

T. Partition：为了实现扩展性，⼀个⾮常⼤的topic可以分布到多个broker（即服务

器）上，⼀个topic可以分为多个partition，每个partition是⼀个有序的队列。

partition中的每条消息都会被分配⼀个有序的id（offset）。kafka只保证按⼀个

partition中的顺序将消息发给consumer，不保证⼀个topic的整体（多个partition

间）的顺序。

V. Offset：kafka的存储⽂件都是按照offset.kafka来命名，⽤offset做名字的好处是⽅

便查找。例如你想找位于2049的位置，只要找到2048.kafka的⽂件即可。当然the

first offset就是00000000000.kafka

2.Consumer与topic关系

本质上kafka只⽀持Topic；

每个group中可以有多个consumer，每个consumer属于⼀个consumer group；

15

16

17

18

19

20

21

22

while (!cur.hasNext) {

if (!self.hasNext) return false

nextCur()

}

true

}

def next(): B =<span style="color:#ffffff"> <span style="background-

}

2020/7/15 2020⼤数据⾯试题真题总结(附答案)

https://mp.weixin.qq.com/s/Lo3zH5fbLs60ZhzDtTtHFw 36/59

通常情况下，⼀个group中会包含多个consumer，这样不仅可以提⾼topic中消息的并发

消费能⼒，⽽且还能提⾼"故障容错"性，如果group中的某个consumer失效那么其消费的

partitions将会有其他consumer⾃动接管。

对于Topic中的⼀条特定的消息，只会被订阅此Topic的每个group中的其中⼀个

consumer消费，此消息不会发送给⼀个group的多个consumer；

那么⼀个group中所有的consumer将会交错的消费整个Topic，每个group中consumer消

息消费互相独⽴，我们可以认为⼀个group是⼀个"订阅"者。

在kafka中,⼀个partition中的消息只会被group中的⼀个consumer消费(同⼀时刻)；

⼀个Topic中的每个partions，只会被⼀个"订阅者"中的⼀个consumer消费，不过⼀个

consumer可以同时消费多个partitions中的消息。

kafka的设计原理决定,对于⼀个topic，同⼀个group中不能有多于partitions个数的

consumer同时消费，否则将意味着某些consumer将⽆法得到消息。

kafka只能保证⼀个partition中的消息被某个consumer消费时是顺序的；事实上，从

Topic⻆度来说,当有多个partitions时,消息仍不是全局有序的。

3.kafka中⽣产数据的时候，如何保证写⼊的容错性？

设置发送数据是否需要服务端的反馈,有三个值0,1,-1

0: producer不会等待broker发送ack

1: 当leader接收到消息之后发送ack

-1: 当所有的follower都同步消息成功后发送ack

request.required.acks=0

4.如何保证kafka消费者消费数据是全局有序的

伪命题

每个分区内，每条消息都有⼀个offset，故只能保证分区内有序。

如果要全局有序的，必须保证⽣产有序，存储有序，消费有序。

由于⽣产可以做集群，存储可以分⽚，消费可以设置为⼀个consumerGroup，要保证全

局有序，就需要保证每个环节都有序。

只有⼀个可能，就是⼀个⽣产者，⼀个partition，⼀个消费者ˇˇˇˇˇˇˇˇˇˇˇ。这种场景和⼤数据应⽤场

景相悖。

5.有两个数据源，⼀个记录的是⼴告投放给⽤户的⽇志，⼀个记录⽤户访问⽇志，另外还有⼀

个固定的⽤户基础表记录⽤户基本信息（⽐如学历，年龄等等）。现在要分析⼴告投放对与哪

类⽤户更有效，请采⽤熟悉的技术描述解决思路。另外如果两个数据源都是实时数据源（⽐如

来⾃kafka），他们数据在时间上相差5分钟，需要哪些调整来解决实时分析问题？

6.Kafka和SparkStreaing如何集成?

7.列举Kafka的优点，简述Kafka为什么可以做到每秒数⼗万甚⾄上百万消息的⾼效分发？

8.为什么离线分析要⽤kafka？

Kafka的作⽤是解耦，如果直接从⽇志服务器上采集的话，实时离线都要采集，等于要采

集两份数据，⽽使⽤了kafka的话，只需要从⽇志服务器上采集⼀份数据，然后在kafka中

使⽤不同的两个组读取就⾏了

9.Kafka怎么进⾏监控?

2020/7/15 2020⼤数据⾯试题真题总结(附答案)

https://mp.weixin.qq.com/s/Lo3zH5fbLs60ZhzDtTtHFw 37/59

Kafka Manager

10.Kafka与传统的消息队列服务有很么不同

11.Kafka api low-level与high-level有什么区别，使⽤low-level需要处理哪些细节

12.Kafka的ISR副本同步队列

ISR（In-Sync Replicas），副本同步队列。ISR中包括Leader和Follower。如果Leader进

程挂掉， 会在ISR 队列中选择⼀个服务作为新的Leader 。有

replica.lag.max.messages（延迟条数）和replica.lag.time.max.ms（延迟时间）两个参

数决定⼀台服务是否可以加⼊ISR 副本队列， 在0.10 版本移除了

replica.lag.max.messages参数，防⽌服务频繁的进去队列。

任意⼀个维度超过阈值都会把Follower剔除出ISR，存⼊OSR（Outof-Sync Replicas）列

表，新加⼊的Follower也会先存放在OSR中。

13.Kafka消息数据积压，Kafka消费能ˇˇˇˇˇˇˇˇˇˇˇ⼒不⾜怎么处理？

1）如果是Kafka消费能⼒不⾜，则可以考虑增加Topic的分区数，并且同时提升消费组的

消费者数量，消费者数=分区数。（两者缺⼀不可）

2）如果是下游的数据处理不及时：提⾼每批次拉取的数量。批次拉取数据过少（拉取数

据/处理时间<⽣产速度），使处理的数据⼩于⽣产的数据，也会造成数据积压。

14.Kafka中的ISR、AR⼜代表什么？

ISR：in-sync replica set (ISR)，与leader保持同步的follower集合

AR：分区的所有副本

15.Kafka中的HW、LEO等分别代表什么？

LEO：每个副本的最后条消息的offset

HW：⼀个分区中所有副本最⼩的offset

16.哪些情景会造成消息漏消费？

先提交offset，后消费，有可能造成数据的重复

17.当你使⽤kafka-topics.sh创建了⼀个topic之后，Kafka背后会执⾏什么逻辑？

1 ） 会在zookeeper 中的/brokers/topics 节点下创建⼀个新的topic 节点，

如：/brokers/topics/first

2）触发Controller的监听程序

3）kafka Controller 负责topic的创建⼯作，并更新metadata cache

18.topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那⼜是为什么？

可以增加

bin/kafka-topics.sh --zookeeper localhost:2181/kafka --alter --topic topic-config --

partitions 3

19.topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那⼜是为什么？

不可以减少，被删除的分区数据难以处理。

20.Kafka有内部的topic吗？如果有是什么？有什么所⽤？

__consumer_offsets,保存消费者offset

21.聊⼀聊Kafka Controller的作⽤？

负责管理集群broker的上下线，所有topic的分区副本分配和leader选举等⼯作。

2020/7/15 2020⼤数据⾯试题真题总结(附答案)

https://mp.weixin.qq.com/s/Lo3zH5fbLs60ZhzDtTtHFw 38/59

22.失效副本是指什么？有那些应对措施？

不能及时与leader同步，暂时踢出ISR，等其追上leader之后再重新加⼊

五.Hbase

1.Hbase调优

⾼可⽤

在HBase中Hmaster负责监控RegionServer的⽣命周期，均衡RegionServer的负载，如果

Hmaster挂掉了，那么整个HBase集群将陷⼊不健康的状态，并且此时的⼯作状态并不会维持太

久。所以HBase⽀持对Hmaster的⾼可⽤配置。

预分区

每⼀个region维护着startRow与endRowKey，如果加⼊的数据符合某个region维护

的rowKey范围，则该数据交给这个region 维护。那么依照这个原则，我们可以将数

据所要投放的分区提前⼤致的规划好，以提⾼HBase性能。

优化RowKey设计

⼀条数据的唯⼀标识就是rowkey，那么这条数据存储于哪个分区，取决于rowkey处

于哪个⼀个预分区的区间内，设计rowkey 的主要⽬的 ，就是让数据均匀的分布于

所有的region中，在⼀定程度上防⽌数据倾斜

内存优化

HBase操作过程中需要⼤量的内存开销，毕竟Table是可以缓存在内存中的，⼀般会分配整个可⽤

内存的70%给HBase的Java堆。但是不建议分配⾮常⼤的堆内存，因为GC过程持续太久会导致

RegionServer处于⻓期不可⽤状态，⼀般16~48G内存就可以了，如果因为框架占⽤内存过⾼导致

系统内存不⾜，框架⼀样会被系统服务拖死。

2.hbase的rowkey怎么创建好？列族怎么创建⽐较好？

hbase存储时，数据按照Row key的字典序(byte order)排序存储。设计key时，要充分排

序存储这个特性，将经常⼀起读取的⾏存储放到⼀起。(位置相关性)

⼀个列族在数据底层是⼀个⽂件，所以将经常⼀起查询的列放到⼀个列族中，列族尽量

少，减少⽂件的寻址时间。

设计原则

1）ˇˇˇˇˇˇˇˇrowkey ⻓度原则

2）rowkey 散列原则

3）rowkey 唯⼀原则

如何设计

1）⽣成随机数、hash、散列值

2）字符串反转

3) 字符串拼接

3.hbase过滤器实现⽤途

增强hbase查询数据的功能

减少服务端返回给客户端的数据量

2020/7/15 2020⼤数据⾯试题真题总结(附答案)

https://mp.weixin.qq.com/s/Lo3zH5fbLs60ZhzDtTtHFw 39/59

4.HBase宕机如何处理

答：宕机分为HMaster宕机和HRegisoner宕机，如果是HRegisoner宕机，HMaster会将

其所管理的region重新分布到其他活动的RegionServer上，由于数据和⽇志都持久在

HDFS中，该操作不会导致数据丢失。所以数据的⼀致性和安全性是有保障的。

如果是HMaster宕机，HMaster没有单点问题，HBase中可以启动多个HMaster，通过

Zookeeper的Master Election机制保证总有⼀个Master运⾏。即ZooKeeper会保证总会

有⼀个HMaster在对外提供服务。

5.hive跟hbase的区别是？

共同点：

1.hbase与hive都是架构在hadoop之上的。都是⽤hadoop作为底层存储

区别：

2.Hive是建⽴在Hadoop之上为了减少MapReduce jobs编写⼯作的批处理系统，HBase

是为了⽀持弥补Hadoop对实时操作的缺陷的项⽬ 。

3.想象你在操作RMDB数据库，如果是全表扫描，就⽤Hive+Hadoop,如果是索引访问，

就⽤HBase+Hadoop 。

4.Hive query就是MapReduce jobs可以从5分钟到数⼩时不⽌，HBase是⾮常⾼效的，肯

定⽐Hive⾼效的多。

5.Hive本身不存储和计算数据，它完全依赖于HDFS和MapReduce，Hive中的表纯逻辑。

6.hive借⽤hadoop的MapReduce来完成⼀些hive中的命令的执⾏

7.hbase是物理表，不是逻辑表，提供⼀个超⼤的内存hash表，搜索引擎通过它来存储索

引，⽅便查询操作。

8.hbase是列存储。

9.hdfs作为底层存储，hdfs是存放⽂件的系统，⽽Hbase负责组织⽂件。

10.hive需要⽤到hdfs存储⽂件，需要⽤到MapReduce计算框架。

6.hbase写流程

1/ 客户端要连接zookeeper, 从zk 的/hbase 节点找到hbase:meta 表所在的

regionserver（host:port）;

2020/7/15 2020⼤数据⾯试题真题总结(附答案)

https://mp.weixin.qq.com/s/Lo3zH5fbLs60ZhzDtTtHFw 40/59

2/ regionserver扫描hbase:meta中的每个region的起始⾏健，对⽐r000001这条数据在

那个region的范围内；

3/ 从对应的 info:server key中存储了region是有哪个regionserver(host:port)在负责的；

4/ 客户端直接请求对应的regionserver；

5/ regionserver接收到客户端发来的请求之后，就会将数据写⼊到region中

7.hbase读流程

1/ ⾸先Client连接zookeeper, 找到hbase:meta表所在的regionserver;

2/ 请求对应的regionserver，扫描hbase:meta表，根据namespace、表名和rowkey在

meta表中找到r00001所在的region是由那个regionserver负责的；

3/找到这个region对应的regionserver

4/ regionserver收到了请求之后，扫描对应的region返回数据到Client

(先从MemStore找数据，如果没有，再到BlockCache⾥⾯读；BlockCache还没有，再到

StoreFile上读(为了读取的效率)；

如果是从StoreFile⾥⾯读取的数据，不是直接返回给客户端，⽽是先写⼊BlockCache，

再返回给客户端。)

8.hbase数据flush过程

1）当MemStore数据达到阈值（默认是128M，⽼版本是64M），将数据刷到硬盘，将内

存中的数据删除，同时删除HLog中的历史数据；

2）并将数据存储到HDFS中；

3）在HLog中做标记点。

9.数据合并过程

N. 当数据块达到4块，hmaster将数据块加载到本地，进⾏合并

O. 当合并的数据超过256M，进⾏拆分，将拆分后的region分配给不同的hregionserver

管理

2020/7/15 2020⼤数据⾯试题真题总结(附答案)

https://mp.weixin.qq.com/s/Lo3zH5fbLs60ZhzDtTtHFw 41/59

P. 当hregionser宕机后， 将hregionserver上的hlog拆分， 然后分配给不同的

hregionserver加载，修改.META.

Q. 注意：hlog会同步到hdfs

10.Hmaster和Hgionserver职责

Hmaster

1、管理⽤户对Table的增、删、改、查操作；

2、记录region在哪台Hregion server上

3、在Region Split后，负责新Region的分配；

4、新机器加⼊时，管理HRegion Server的负载均衡，调整Region分布

5、在HRegion Server宕机后，负责失效HRegion Server 上的Regions迁移。

Hgionserver

HRegion Server主要负责响应⽤户I/O请求，向HDFS⽂件系统中读写数据，是HBASE中

最核⼼的模块。

HRegion Server管理了很多table的分区，也就是region。

11.HBase列族和region的关系？

HBase有多个RegionServer，每个RegionServer⾥有多个Region，⼀个Region中存放着

若⼲⾏的⾏键以及所对应的数据，⼀个列族是⼀个⽂件夹，如果经常要搜索整个⼀条数

据，列族越少越好，如果只有⼀部分的数据需要经常被搜索，那么将经常搜索的建⽴⼀个

列族，其他不常搜索的建⽴列族检索较快。

12.请简述Hbase的物理模型是什么

13.请问如果使⽤Hbase做即席查询，如何设计⼆级索引

14.如何避免读、写HBaes时访问热点问题？

（1）加盐

这⾥所说的加盐不是密码学中的加盐，⽽是在rowkey的前⾯增加随机数，具体就是给

rowkey分配⼀个随机前缀以使得它和之前的rowkey的开头不同。给多少个前缀？这个数

量应该和我们想要分散数据到不同的region的数量⼀致（类似hive⾥⾯的分桶）。

（ ⾃⼰理解：即region数量是⼀个范围，我们给rowkey分配⼀个随机数，前缀（随机

数）的范围是region的数量）

加盐之后的rowkey就会根据随机⽣成的前缀分散到各个region上，以避免热点。

（2）哈希

哈希会使同⼀⾏永远⽤⼀个前缀加盐。哈希也可以使负载分散到整个集群，但是读却是可

以预测的。使⽤确定的哈希可以让客户端重构完整的rowkey，可以使⽤get操作准确获取

某⼀个⾏数据。

（3）反转

第三种防⽌热点的⽅法是反转固定⻓度或者数字格式的rowkey。这样可以使得rowkey中

经常改变的部分（最没有意义的部分）放在前⾯。这样可以有效的随机rowkey，但是牺

牲了rowkey的有序性。反转rowkey的例⼦：以⼿机号为rowkey，可以将⼿机号反转后的

字符串作为rowkey，从⽽避免诸如139、158之类的固定号码开头导 致的热点问题。

2020/7/15 2020⼤数据⾯试题真题总结(附答案)

https://mp.weixin.qq.com/s/Lo3zH5fbLs60ZhzDtTtHFw 42/59

（4）时间戳反转

⼀个常⻅的数据处理问题是快速获取数据的最近版本，使⽤反转的时间戳作为rowkey的

⼀部分对这个问题⼗分有⽤，可以⽤Long.Max_Value – timestamp追加到key的末尾，例

如[key][reverse_timestamp] ,[key] 的最新值可以通过scan [key]获得[key]的第⼀条记

录，因为HBase中rowkey是有序的，第⼀条记录是最后录⼊的数据。

（5）尽量减少⾏和列的⼤⼩

在HBase中，value永远和它的key⼀起传输的。当具体的值在系统间传输时，它的

rowkey，列名，时间戳也会⼀起传输。如果你的rowkey和列名很⼤，HBase storefiles中

的索引（有助于随机访问）会占据HBase分配的⼤量内存，因为具体的值和它的key很

⼤。可以增加block⼤⼩使得storefiles索引再更⼤的时间间隔增加，或者修改表的模式以

减⼩rowkey和列名的⼤⼩。压缩也有助于更⼤的索引。

（6）其他办法

列族名的⻓度尽可能⼩，最好是只有⼀个字符。冗⻓的属性名虽然可读性好，但是更短的

属性名存储在HBase中会更好。也可以在建表时预估数据规模，预留region数量，例如

create ‘myspace:mytableʼ, SPLITS => [01,02,03,,…99]

15.布隆过滤器在HBASE中的应⽤

16.Hbase是⽤来⼲嘛的?什么样的数据会放到hbase

六.数仓

1.维表和宽表的考查（主要考察维表的使⽤及维度退化⼿法）

维表数据⼀般根据ods层数据加⼯⽣成，在设计宽表的时候，可以适当的⽤⼀些维度退化

⼿法，将维度退化到事实表中，减少事实表和维表的关联

2.数仓表命名规范

每个公司都会有点差别

ODS

ods.库名_表名_df/di/da/dz

CDM(dwd/dws)

dwd.主题_内容_df

3.拉链表的使⽤场景

1.数据量⽐较⼤

2.表中的部分字段会被更新

3.需要查看某⼀个时间点或者时间段的历史快照信息

查看某⼀个订单在历史某⼀个时间点的状态

某⼀个⽤户在过去某⼀段时间，下单次数

4.更新的⽐例和频率不是很⼤

如果表中信息变化不是很⼤，每天都保留⼀份全量，那么每次全量中会保存很多不

变的信息，对存储是极⼤的浪费

4.⼀亿条数据查的很慢,怎么查快⼀点

5.有什么维表

2020/7/15 2020⼤数据⾯试题真题总结(附答案)

https://mp.weixin.qq.com/s/Lo3zH5fbLs60ZhzDtTtHFw 43/59

时间维表，⽤户维表，医院维表等

6.数据源都有哪些

业务库数据源:mysql,oracle,mongo

⽇志数据：ng⽇志，埋点⽇志

爬⾍数据

7.你们最⼤的表是什么表,数据量多少

ng⽇志表，三端(app,web,h5)中app端⽇志量最⼤，清洗⼊库后的数据⼀天⼤概xxxxW

8.数仓架构体系

9.数据平台是怎样的，⽤到了阿⾥的那⼀套吗？

没⽤到阿⾥那⼀套，数据平台为⾃研产品

10.你了解的调度系统有那些？，你们公司⽤的是哪种调度系统

airflow，azkaban，ooize，我们公司使⽤的是airflow

11.你们公司数仓底层是怎么抽数据的？

业务数据⽤的是datax

⽇志数据⽤的是logstash

12.为什么datax抽数据要⽐sqoop 快？

13.埋点数据你们是怎样接⼊的

logstash-->kafka-->logstash-->hdfs

14.如果你们业务库的表有更新，你们数仓怎么处理的？

根据表数据量及表特性，选择⽤全量表，增量表，追加表和拉链表处理

15.能独⽴搭建数仓吗

可以

16.搭建过CDH 集群吗

17.说⼀下你们公司的⼤数据平台架构？你有参与吗？

2020/7/15 2020⼤数据⾯试题真题总结(附答案)

https://mp.weixin.qq.com/s/Lo3zH5fbLs60ZhzDtTtHFw 44/59

18.介绍⼀下你⾃⼰的项⽬和所⽤的技术

19.对⽬前的流和批处理的认识？就是谈谈⾃⼰的感受

20.你了解那些OLAP 引擎，MPP 知道⼀些吗？clickHouse 了解⼀些吗？你⾃⼰做过测试性能

吗？

21.Kylin 有了解吗？介绍⼀下原理

22.datax 源码有改造过吗

改造过

23.你们数仓的APP 层是怎么对外提供服务的？

1.直接存⼊mysql业务库，业务⽅直接读取

2.数据存⼊mysql，以接⼝的形式提供数据

3.数据存⼊kylin，需求⽅通过jdbc读取数据

24.数据接⼊进来，你们是怎样规划的，有考虑数据的膨胀问题吗

25.简述拉链表，流⽔表以及快照表的含义和特点

拉链表：

（1）记录⼀个事物从开始，⼀直到当前状态的所有变化的信息；

（2）拉链表每次上报的都是历史记录的最终状态，是记录在当前时刻的历史总ˇˇˇˇˇˇˇˇˇˇˇˇˇˇ量；

（3）当前记录存的是当前时间之前的所有历史记录的最后变化量（总量）；

（4）封链时间可以是2999，3000，9999等等⽐较⼤的年份；拉链表到期数据要报0；

流⽔表：对于表的每⼀个修改都会记录，可以⽤于反映实际记录的变更

区别于拉链表：

拉链表通常是对账户信息的历史变动进⾏处理保留的结果，流⽔表是每天的交易形成的

历史；

流⽔表⽤于统计业务相关情况，拉链表⽤于统计账户及客户的情况

快照表：

按天分区，每⼀天的数据都是截⽌到那⼀天mysql的全量数据

26.全量表(df),增量表(di),追加表(da)，拉链表(dz)的区别及使⽤场景

27.你们公司的数仓分层，每⼀层是怎么处理数据的

2020/7/15 2020⼤数据⾯试题真题总结(附答案)

https://mp.weixin.qq.com/s/Lo3zH5fbLs60ZhzDtTtHFw 45/59

28.什么是事实表，什么是维表

29.星型模型和雪花模型

30.数据建模⼀般有哪⼏种⽅式，你们公司是⽤哪种⽅式进⾏数据建模的

ER模型

维度模型

Data Vault模型

Anchor模型

我司⽤的是维度建模

31.有没有实际⼯作中碰到的sql调优的例⼦，举例说明

32.你觉得数据仓库应该如何搭建，数据规范和标准如何落地

33.如何保证你们公司的数据质量

34.对元数据的理解，元数据管理的意义及应⽤场景有哪些

35.如何判别模型的好坏，模型设计的原则有哪些

基本原则：

⾼内聚和低耦合

⼀个逻辑或者物理模型由哪些记录和字段组成，应该遵循最基本的软件设计⽅法论的⾼内

聚和低耦合原则，主要从数据业务特性和访问特性两个⻆度来考虑：将业务相近或者相

关，粒度相同的数据设计为⼀个逻辑或者物理模型，将⾼概率同时访问的数据放⼀起，将

低概率同时访问的数据分开存储

核⼼模型与扩展模型分离

建⽴核⼼模型与扩展模型体系，核⼼模型包括的字段⽀持常⽤的核⼼业务，扩展模型包括

的字段⽀持个性化或少量应⽤的需要，不能让扩展模型的字段过度侵⼊核⼼模型，以免破

坏核⼼模型架构简洁性与可维护性

公共处理逻辑下沉及单⼀

越是底层共⽤的处理逻辑越应该放在数据调度依赖的底层进⾏封装与实现，不要让共⽤的

处理逻辑暴露给应⽤层实现，不要让公共逻辑多处同时存在

成本与性能平衡

适当的数据冗余可换取查询和刷新性能，不宜过度数据冗余和数据复制

数据可回滚

处理逻辑不变，在不同时间多次运⾏数据结果确定不变

⼀致性

具有相同含义的字段在不同的表中的命名必须相同，必须使⽤规范定义中的名称

命名清晰,可理解

2020/7/15 2020⼤数据⾯试题真题总结(附答案)

https://mp.weixin.qq.com/s/Lo3zH5fbLs60ZhzDtTtHFw 46/59

表命名需清晰，⼀致，表名需易于消费者理解和使⽤

36.对于数据中台的理解，和数据仓库，数据湖的区别

37.对于数据仓库的理解，数据仓库主要为解决什么问题

38.数据仓库模型的理解，数据仓库分层设计的好处是什么

清晰数据结构：每⼀个数据分层都有它的作⽤域和职责，在使⽤表的时候能更⽅便地定位

和理解

减少重复开发：规范数据分层，开发⼀些通⽤的中间层数据，能减少极⼤的重复计算

统⼀数据⼝径：通过数据分层提供统⼀的数据出⼝，统⼀对外输出的数据⼝径

复杂问题简单化：将⼀个复杂的任务分解成多个步骤来完成，每⼀层解决特定的问题

39.数仓主题划分的标准和依据

40.缓慢变化维如何处理，⼏种⽅式

七.Flink

1.Flink实时计算时落磁盘吗

不落，是内存计算

2.⽇活DAU的统计需要注意什么

3.Flink调优

4.Flink的容错是怎么做的

定期checkpoint存储oprator state及keyedstate到stateBackend

5.Parquet格式的好处？什么时候读的快什么时候读的慢

6.flink中checkPoint为什么状态有保存在内存中这样的机制？为什么要开启checkPoint?

开启checkpoint可以容错，程序⾃动重启的时候可以从checkpoint中恢复数据

7.flink保证Exactly_Once的原理？

1.开启checkpoint

2.source⽀持数据重发

3.sink⽀持事务，可以分2次提交，如kafka；或者sink⽀持幂等，可以覆盖之前写⼊的数

据，如redis

满⾜以上三点，可以保证Exactly_Once

8.flink的时间形式和窗⼝形式有⼏种？有什么区别，你们⽤在什么场景下的？

9.flink的背压说下？

10.flink的watermark机制说下，以及怎么解决数据乱序的问题？

11.flink on yarn执⾏流程

2020/7/15 2020⼤数据⾯试题真题总结(附答案)

https://mp.weixin.qq.com/s/Lo3zH5fbLs60ZhzDtTtHFw 47/59

Flink任务提交后，Client向HDFS上传Flink的Jar包和配置，之后向Yarn

ResourceManager提交任务，ResourceManager分配Container资源并通知对应的

NodeManager启动ApplicationMaster，ApplicationMaster启动后加载Flink的Jar包和配

置构建环境，然后启动JobManager，之后ApplicationMaster向ResourceManager申请

资源启动TaskManager ， ResourceManager 分配Container 资源后， 由

ApplicationMaster 通知资源所在节点的NodeManager 启动TaskManager ，

NodeManager加载Flink的Jar包和配置构建环境并启动TaskManager，TaskManager启

动后向JobManager发送⼼跳包，并等待JobManager向其分配任务。

12.说⼀说spark 和flink 的区别

⼋.Java

1.hashMap底层源码，数据结构

2.写出你⽤过的设计模式，并举例说明解决的实际问题

3.Java创建线程的⼏种⽅式

N. 继承Thread类，重写run⽅法

O. 实现Runnable接⼝，实现run⽅法

P. 通过线程池获取线程

Q. 实现Callable接⼝并实现call⽅法，创建该类的实例，使⽤FutureTask类包装Callable

对象，使⽤FutureTask对象作为Thread对象的target创建并启⽤新线程

4.请简述操作系统的线程和进程的区别

5.Java程序出现OutOfMemoryError:unable to create new native thread 的原因可能有哪

些？如何分析和解决？

6.采⽤java或⾃⼰熟悉的任何语⾔分别实现简单版本的线性表和链表，只需实现add,remove

⽅法即可

7.ArrayList和LinkedList的区别

8.JVM 内存分哪⼏个区，每个区的作⽤是什么?

9.Java中迭代器和集合的区别？

集合是将所有数据加载到内存，然后通过集合的⽅法去内存中获取，⽽迭代器是⼀个对

象，实现了Iterator接⼝，实现了接⼝的hasNext和Next⽅法。

2020/7/15 2020⼤数据⾯试题真题总结(附答案)

https://mp.weixin.qq.com/s/Lo3zH5fbLs60ZhzDtTtHFw 48/59

10.HashMap 和 HashTable 区别

1) 线程安全性不同

HashMap 是线程不安全的，HashTable 是线程安全的，其中的⽅法是 Synchronize 的，

在多线程并发的情况下，可以直接使⽤ HashTabl，但是使⽤ HashMap 时必须⾃⼰增加

同步

处理。

2) 是否提供 contains ⽅法

HashMap 只有 containsValue 和 containsKey ⽅法；HashTable 有 contains 、

containsKey

和 containsValue 三个⽅法，其中 contains 和 containsValue ⽅法功能相同。

3) key 和 value 是否允许 null 值

Hashtable 中，key 和 value 都不允许出现 null 值。HashMap 中，null 可以作为键，这

样的键只有⼀个；可以有⼀个或多个键所对应的值为 null。

4) 数组初始化和扩容机制

HashTable 在不指定容量的情况下的默认容量为 11，⽽ HashMap 为 16，Hashtable 不

要求底层数组的容量⼀定要为 2 的整数次幂，⽽ HashMap 则要求⼀定为 2 的整数次

幂。

Hashtable 扩容时，将容量变为原来的 2 倍加 1，⽽ HashMap 扩容时，将容量变为原

来的 2 倍。

11.线程池使⽤注意哪些⽅⾯？

线程池分为单线程线程池，固定⼤⼩线程池，可缓冲的线程池

12.HashMap和TreeMap的区别？TreeMap排序规则？

TreeMap会⾃动进⾏排序，根据key的Compare⽅法进⾏排序

13.⽤java实现单例模式

14.使⽤递归算法求n的阶乘：n! ,语⾔不限

15.HashMap和Hashtable的区别是什么

16.TreeSet 和 HashSet 区别

HashSet 是采⽤ hash 表来实现的。其中的元素没有按顺序排列，add()、remove()以及

contains()等⽅法都是复杂度为 O(1)的⽅法。

TreeSet 是采⽤树结构实现（红⿊树算法）。元素是按顺序进⾏排列，但是 add()、

remove()以及 contains()等⽅法都是复杂度为 O(log (n))的⽅法。它还提供了⼀些⽅法来

处理

排序的 set，如 first()，last()，headSet()，tailSet()等等。

17.Stringbuffer 和 Stringbuild 区别

1、StringBuffer 与 StringBuilder 中的⽅法和功能完全是等价的。

2、只是 StringBuffer 中的⽅法⼤都采⽤了 synchronized 关键字进⾏修饰，因此是线程

安全的，⽽ StringBuilder 没有这个修饰，可以被认为是线程不安全的。

3、在单线程程序下，StringBuilder 效率更快，因为它不需要加锁，不具备多线程安全

⽽ StringBuffer 则每次都需要判断锁，效率相对更低

2020/7/15 2020⼤数据⾯试题真题总结(附答案)

https://mp.weixin.qq.com/s/Lo3zH5fbLs60ZhzDtTtHFw 49/59

18.Final、Finally、Finalize

final：修饰符（关键字）有三种⽤法：修饰类、变量和⽅法。修饰类时，意味着它不

能再派⽣出新的⼦ˇˇˇˇˇˇˇˇ类，即不能被继承，因此它和 abstract 是反义词。修饰变量时，该变

量

使⽤中不被改变，必须在声明时给定初值，在引⽤中只能读取不可修改，即为常量。修饰

⽅法时，也同样只能使⽤，不能在⼦类中被重写。

finally：通常放在 try…catch 的后⾯构造最终执⾏代码块，这就意味着程序⽆论正常执

⾏还是发⽣异常，这⾥的代码只要 JVM 不关闭都能执⾏，可以将释放外部资源的代码写

在

finally 块中。

finalize：Object 类中定义的⽅法，Java 中允许使⽤ finalize() ⽅法在垃圾收集器将对象

从内存中清除出去之前做必要的清理⼯作。这个⽅法是由垃圾收集器在销毁对象时调⽤

的，通过重写 finalize() ⽅法可以整理系统资源或者执⾏其他清理⼯作。

19..==和 Equals 区别

== : 如果⽐较的是基本数据类型，那么⽐较的是变量的值

如果⽐较的是引⽤数据类型，那么⽐较的是地址值（两个对象是否指向同⼀块内

存）

equals:如果没重写 equals ⽅法⽐较的是两个对象的地址值。

如果重写了 equals ⽅法后我们往往⽐较的是对象中的属性的内容

equals ⽅法是从 Object 类中继承的，默认的实现就是使⽤==

20.⽐较ArrayList，LinkedList的存储特性和读写性能

21.Java 类加载过程

Java类加载需要经历⼀下⼏个过程：

加载

加载时类加载的第⼀个过程，在这个阶段，将完成⼀下三件事情：

N. 通过⼀个类的全限定名获取该类的⼆进制流。

O. 将该⼆进制流中的静态存储结构转化为⽅法去运⾏时数据结构。

P. 在内存中⽣成该类的Class对象，作为该类的数据访问⼊⼝。

验证

验证的⽬的是为了确保Class⽂件的字节流中的信息不回危害到虚拟机.在该阶段主要完成

以下四钟验证:

N. ⽂件格式验证：验证字节流是否符合Class⽂件的规范，如主次版本号是否在当前虚

拟机范围内，常量池中的常量是否有不被⽀持的类型.

O. 元数据验证:对字节码描述的信息进⾏语义分析，如这个类是否有⽗类，是否集成了

不被继承的类等。
